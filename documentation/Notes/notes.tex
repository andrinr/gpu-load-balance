\documentclass[]{article}

% Packages

% Pseudocode
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{arrayjob}

% Placing
\usepackage{float}
% Drawing
\usepackage{tikz}
\usetikzlibrary{arrows.meta,chains,%
	decorations.pathreplacing}
% Memory maps
\usepackage{bytefield}
% Simple trees
\usepackage{qtree}
% Better trees
\usepackage{forest} 
% Custom colors
\usepackage{xcolor}
% Comments
\usepackage{verbatim}
% math symbols
\usepackage{amssymb}
% csv reader
\usepackage{csvsimple}
% create file content
\usepackage{filecontents}
% Create c++ fields
\usepackage{listings}
% better tables
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}

% Plots
\usepackage{pgfplots}
\pgfplotsset{compat = newest}
\usepgfplotslibrary{fillbetween}

% Dependencies
\usepackage{mymacros}
\usepackage{parallelP}

\lstset { %
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
    numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
    numbersep=5pt,                   % how far the line-numbers are from the code
    numberstyle=\tiny\color{black}, % the style that is used for the line-numbers
    rulecolor=\color{black},
    keywordstyle=\color{blue},
    tabsize=3,	
}


% Title
\title{Orthogonal Recursive Bisection on the GPU for Accelerated Load Balancing in Large N-Body Simulations \\ - \\ Bachelor Thesis}

\author{Andrin Rehmann}

% Start
\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage
\section{Introduction}

The N-Body technique has been used for decades to simulate the Universe so we can compare theory with observations. This technique uses ``particles'' or ``bodies'' to sample phase space, and as gravity operates over infinite distance it is necessary to consider all pair-wise interactions which makes a naive implementation $\mathcal{O}(n^2)$.
It is clear that this does not scale particularly well with large particle counts.

One common approach is to decompose the particles into a tree structure and multipole expansions of the particles in each tree cell to approximate the forces. This reduces the complexity of the algorithm to $\mathcal{O}(n\log{}n)$. More recently the Fast Multipole Method (FMM), also relying on the tree data structure, has gained wider option primarily due to the extremely large sizes of modern simulations. This technique further reduces the complexity to $\mathcal{O}(n)$. 

The computational effort required in modern N-Body simulations can be split into three categories:

\begin{itemize}
	\item \textbf{Load Balancing:} Distribute particles equally among nodes with regards to memory.
	\item \textbf{Tree Building:} Build tree on each node for accelerated force calculation and integration.
	\item \textbf{Force calculation and integration:} Leverage the tree and FFM to calculate forces between particles and apply them.
\end{itemize}

Before the implementation of FMM into codes, and particularly before the era of modern accelerated computing (e.g., SIMD vectorization and GPU computing) the forces calculations dominated the computational cost. In more recent simulations, each category is about one third of the total calculation time\cite{2017ComAC...4....2P}. This makes the tree building and load balancing subjects to great performance gains, since GPU acceleration is usually not exploited. 

This project proposes to implement Tree Building with the GPU using CUDA to accelerate load balancing. We will consider implementing the same method for tree building as well. Thus some considerations, which are important for FFM, also apply to this work.

\subsection{Target Data}\label{section:target-data}

\TODO{Write about properties of datasets}

The target data consists of $N$ particles where each particle has several data points and introduce the following definitions:

\begin{itemize}
	\item We define a particle as $p_i$  where $i \in \{0,...,N\}$. 
	\item We define the space of binary numbers with a precision of $p$ as $\mathbb{B}_p$ where we have $a \in \mathbb{B}_p \Leftrightarrow a \in \{0,1\}^{p}$
	\item We define the corner coordinates of root domain with $\vec{lower}, \vec{upper}$ where we have $\vec{lower}, \vec{upper} \in \mathbb{B}_p^3$. 
	\item We define the coordinates of a particle $p_i$ as $\vec{x_i}$ for which holds $\{\vec{x} | \vec{lower} \leq \vec{x} \leq \vec{upper}, \vec{x} \in \mathbb{B}_p^3 \}$.

\end{itemize}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\randistr{10}{5}{100}
			\draw (0,0) rectangle (10, 5);
			\filldraw  (0,0) circle (3pt) node [anchor=west]{};
			\node[yshift=0.3cm, xshift=-0.3cm] at (0,0) {$-\frac{b}{2}$};
			
			\filldraw  (10,5) circle (3pt) node [anchor=west]{};
			\node[yshift=0.3cm, xshift=-0.3cm] at (10,5) {$\frac{b}{2}$};
		\end{tikzpicture}
	\end{center}
\caption{Unfiorm random distribution of 3D coordinates in cube domain projected onto a 2d plane}
\end{figure}

\Q{Ask Doug}

\subsection{Force Integration and FFM}\label{section:force-integration}

\TODO{Write about how the very unequal data can also affect integration }
Non equal weighting functions for different particles are due to higher accelerations and greater proximity to strong gravitational influences. This in turn requires a higher integrations accuracy to mitigate errors. When we try to balance the workload, this implies drawbacks in terms of memory balance. 



\subsection{Target Computing Systems}\label{section:target-systems}

\TODO{Write about systems which can be used to test data and systems which are worth considering while estimating runtimes of this approach.}

\subsection{PKDGrav}


\newpage
\section{Orthogonal Recursive Bisection (ORB)} \label{section:orb}


%https://de.overleaf.com/learn/latex/TikZ_package
%https://texample.net/tikz/examples/
This section will introduce the ORB algorithm along with its subroutines.

The ORB algorithm partitions a multi-dimensional domain into subdomains based on spatial proximity, by splitting a $k$-dimensional domain containing $N$ particles into $d$ subdomains each of $k$ dimensions. This is most efficiently achieved using a recursive algorithm. During the recursive process, all intermediate domains are stored in a binary tree. A node of this tree is referred to as a $cell$. A $cell$ is a datastructure which keeps track of the domain information and also keeps pointers to its left and right child cells. When the final tree is examined, the number of leaf nodes (or leaf cells) must be equal to $d$. The tree can be leveraged in two ways:
\begin{enumerate}
	\item For simulation code, and more specifically for the Fast Multipole Method, the tree can be used to greatly accelerate the simulation time. \TODO{expand}
	\item The tree can be used to equally distribute memory and/or workload among nodes or processors in a computing system. 
\end{enumerate}

Our main objective in this work is to improve the runtime of astrophysical simulations, or more specifically PKDGrav. At the same time we want to minimize penalties in regards to $N$, the total number of particles, such that we can leverage a high $N$ to increase the precision of the simulation. Furthermore we want to adapt and optimize the algorithms with regards to the target data as introduced in \ref{section:target-data} and target computing systems (\ref{section:target-systems}).

\subsubsection{By example} 
For the purpose of explaining the algorithm both visually and by a numeric example, we consider the following minimal dataset:

\begin{filecontents*}{particles10.csv}
x,y,id,o1,o2,x2,y2
0.4,0.3,0,0,0,0.4,0.3
0.2,0.6,1,2,1,0.2,0.6
0.8,0.9,2,1,2,0.9,0.8
0.6,0.5,3,0,0,0.6,0.5
0.3,0.8,4,1,1,0.3,0.8
0.7,0.1,5,2,2,0.7,0.1
0.9,0.3,6,2,2,0.9,0.3
\end{filecontents*}

\begin{figure}[H]
    \begin{center}
        \begin{minipage}[c]{0.2\linewidth}
        \csvreader[
            tabular=ccc,
            table head=\hline \bfseries{x} & \bfseries{y} & \bfseries{id} \\\hline,
            late after last line=\\\hline % horizontal line at the end of the table
            ]{particles10.csv}{}{\csvcoli & \csvcolii & \csvcoliii}
        \end{minipage}
        \begin{minipage}[c]{0.7\linewidth}
            \begin{tikzpicture}
                \begin{axis}
                 [
                    nodes near coords,
                    xmin=0,
                    xmax=1,
                    ymin=0,
                    ymax=1
                ]
                \addplot+[
                    only marks,
                    point meta=explicit symbolic
                ] table [
                    x=x, 
                    y=y, 
                    meta=id, 
                    col sep=comma] 
                {particles10.csv};
                \end{axis}

            \end{tikzpicture}
        \end{minipage}
    \end{center}
\caption{Example distribution with $N$ = 7}
\end{figure}


\subsection{Memory and Workload Balancing}\label{balancing}
We have now formulated our main objectives for improving ORB, which are (1) to improve the runtime, which is done with workload balancing, whilst (2) at the same time keeping $N$ as large as possible. In this section we will explain the averse effects when disregarding interactions between maximizing runtime and maximizing N.


As described in section \ref{section:force-integration}, if we want to reduce the error across all particles, we need to introduce more discrete time steps for certain particles than for others. The need for non-constant simulations steps across the simulation is caused by the vastly variant forces which are exercised on different particles. It is possible that certain particles follow a straight path with an almost constant velocity, whereas others are influenced by strong gravitational poles. 

We denote the workload for a particle $p_i$ as the weighting function $w(p_i)$. The workload correlates with the number of simulations steps, that need to be computed for a single particle. In an optimally parallelized system, the workload should be very similar among computing units, thus it would appear make sense to distribute the particles regarding the weighting function. This in turn implies an unequal distribution of the particles with regards to memory.

Let us consider a simple example to illustrate the point. Given the set of particles $A = \{p_1, p_2, .., p_{2m/3}\}$ and respectively $B = \{p_{2m/3 + 1}, p_2, .., p_{m}\}$. This gives us a total number of particles equivalent to $N = m$. Let us now consider that $\forall p \in A : w(p) = 1$ and $\forall p \in B : w(p) = 2$. If we assign all particles from set $A$ to process with rank 0 and the particles from B to rank 1. It is clear that $\sum_{p\in A} w(p) = \sum_{p\in B} w(p)$. Thus the two processors are balanced in terms of computing costs, but clearly they are not balanced in terms of memory size. In fact, process with rank 0 has $2m/3$ elements and rank 1 has $m/3$ elements. Assuming each process has a memory size of $2m/3$, then clearly this configuration is not optimal. If we were to favor memory balancing, we could assign $2m/3$ to each processor and have $(4/3) \times m$ particles in total. 

To which degree we decide to favor memory over workload balancing is difficult to answer. But as for now we will parametrize the workload using the weighting function, which allows us to optimize and decide at a later stage. If we decide to completely ignore the workload balancing and solely focus on memory balancing, we can simply set the $w(p_i) = 1$ for all particles.


\subsection{ORB}


In each recursive call of the ORB algorithm, we are given a $cell$ along with a number of desired leaf cells $d_{cell}$, which corresponds to $d$ for the root cell. Furthermore each cell has information about the volume it encompasses (also refereed to its domain) stored as $V_{cell}$. We then split this $cell$, into two child cells $leftCell$ and $rightCell$. For the domains of $leftCell$ and $rightCell$ we have the following properties: 

\begin{center}
	\begin{equation}
		V_{cell} = V_{leftCell} \cup V_{rightChild}
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		V_{leftCell} \cap V_{rightChild} = \emptyset
	\end{equation}
\end{center}

How the volumes of the child cells are exactly defined, will be explained in later stages of this section. Primarily we need to define the variable $d$ for the child cells.

Given a cell, we recursively define $nLeafCells$ for its children as follows:

\begin{center}
	\begin{equation}
		d_{leftCell} = \left \lceil\frac{d_{cell}}{2} \right \rceil 
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		d_{rightCell} = d_{cell} - d_{leftCell}
	\end{equation}
\end{center}

Finally we need to determine the number of particles, which should be encompassed in $V_{leftCell}$ and $V_{rigtCell}$. We define the number of particles in the current cell as $N_{cell}$, respectively the left and right child cells are defined as follows:

\begin{center}
	\begin{equation}
		N_{leftCell} = min \left \{ x \in \{0,...,N_{cell} \} : \sum_{i=0}^{x} w(p_i) \geq \frac{d_{leftCell}}{d_{cell}} \times \sum_{i=0}^{N_{cell}} w(p_i) \right \} 
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		N_{rightCell} = N_{cell} - N_{leftCell}
	\end{equation}
\end{center}

With this formula we make sure, that for any given $d$, we can subdivide the domain accordingly and end up with leaf cells, who encompass an almost equal amount of particles. \TODO{Insert better explanation and example here.}


To simplify the visual and numeric explanation of the ORB algorithm, we assume $\forall i \in \{0,..,N\} : w(p_i) = 1$.

The ORB algorithm is initiated by considering the volume $V_{cell}$ which encompasses all particle coordinates. In each iteration we use a binary search algorithm, which we will introduce in later stages, to determine a cut $c$ position. This cut position is defined along a specified axis, which is determined by the dimension where the domain is largest. In three dimensional space we can then represent this cut as a plane and make simple checks weather a particle is right or left of this plane. 

The ideal cut plane is defined such that exactly $N_{leftChild}$ particles are located to its left side. This cut plane allows us to split the volume $V_{cell}$ in two subdomain, where the $V_{leftCell}$ is constrained by our original domain boundaries and the cut plane on its right side. The same can be done for the right cell, where the cut plane determines the left side boundaries of the volume $V_{rightCell}$. We repeat this process until we have segmented our initial domain into $d$ leaf cells.

As described in \cite{Stadel2001}, "the opening radius of a cell is given by some constant factor times $r_{max}$". $r_{max}$ is the distance between the center of mass of a cell and its most distant corner. The opening radius influences the error ratio, which in turn influences how many calculations have to be performed. Trivially, a square is the rectangle where the distance between any point and its most distant corner point is the smallest. Thus, we will want to approach more square-like shapes.
The most straight forward way to avoid high ratios between the length and height of such a rectangle is to always split it along its longest axis.

    
\begin{figure}[H]

        \centering
            \begin{tikzpicture}
                \begin{axis}
                [
                    nodes near coords,
                    xmin=-0.,
                    xmax=1.,
                    ymin=-0.,
                    ymax=1.,
                ]
                \addplot    +[
                    only marks,
                    point meta=explicit symbolic
                ] table [
                    x=x, 
                    y=y, 
                    meta=id, 
                    col sep=comma] 
                {particles10.csv};
                \draw [fill=red!20](0.0,0.0) rectangle (1, 1);
                \node[below] at (0.5, 1){$cell_1$};
                \end{axis}
            \end{tikzpicture}
\caption{Example particles with ORB at recursion depth 0}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{forest}
        [$cell_1$
        []  
        ]
    \end{forest}    
\caption{Tree with ORB at recursion depth 0}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\pgfmathsetseed{2};
                \begin{axis}
                [
                    nodes near coords,
                    xmin=-0.,
                    xmax=1.,
                    ymin=-0.,
                    ymax=1.,
                ]
                \addplot+[
                    only marks,
                    point meta=explicit symbolic
                ] table [
                    x=x, 
                    y=y, 
                    meta=id, 
                    col sep=comma] 
                {particles10.csv};
                \draw [fill=green!20](0.0,0.0) rectangle (0.65, 1);
                \draw [fill=blue!20](0.65,0.0) rectangle (1, 1);
                \node[below] at (0.2, 1){$cell_2$};
                 \node[below] at (0.7, 1){$cell_3$};
                \end{axis}
            
		\end{tikzpicture}
    \end{center}
\caption{Example particles with ORB at recursion depth 1}
\end{figure}

\begin{figure}[H]
\centering
	\begin{forest}
		[$cell_1$
		[$cell_2$][$cell_3$]  
		]
	\end{forest}    
\caption{Tree with ORB at recursion depth 1}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\pgfmathsetseed{2};
                \begin{axis}
                [
                    nodes near coords,
                    xmin=-0.,
                    xmax=1.,
                    ymin=-0.,
                    ymax=1.,
                    title=Recursion depth 2
                ]
                \addplot+[
                    only marks,
                    point meta=explicit symbolic
                ] table [
                    x=x, 
                    y=y, 
                    meta=id, 
                    col sep=comma] 
                {particles10.csv};
                \draw [fill=pink!20](0.0,0.0) rectangle (0.65, 0.55);
                \draw [fill=yellow!20](0.0,0.55) rectangle (0.65, 1);
                \draw [fill=blue!20](0.65,0.0) rectangle (1, 1);
                \node[below] at (0.2, 1){$cell_4$};
                \node[above] at (0.2, 0){$cell_5$};
                \node[below] at (0.7, 1){$cell_3$};
                \end{axis}
            
		\end{tikzpicture}
    \end{center}
\caption{Example particles with ORB at recursion depth 2}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{forest}
		[$cell_1$
		[$cell_2$ [$cell_4$] [$cell_5$]][$cell_3$]  
		]
	\end{forest}
 \caption{Tree with ORB at recursion depth 2}
\end{figure}


Since $d_{cellLeft} - d_{cellRight}$ must always be $\leq 1$, it must hold that the tree is a nearly complete binary tree. Therefore we can conclude that the height of the tree is equal to $\lceil log(d) \rceil$ and the total number of nodes (not only leaf nodes) is $log(d) \times d$. Since we have a nearly complete binary tree, we also have the possibility of storing it in a heap data-structure, which can in turn greatly increase the performance, since we will not have to perform a tree walk to access individual cells in the tree.

\begin{algorithm}[H]
	\caption{The ORB main routine}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{orb}{$\vec{lower}, \vec{upper} ,x, d_i$}
		\State $\vec{size} = \vec{upper} - \vec{lower}$
		\State $axis = maxIndex(\vec{size}_0, \vec{size}_1, \vec{size}_2)$ \Comment{Get index of max value}
		\newline
		
		\State $cut = cut(x, \vec{lower}_{axis}, \vec{upper}_{axis}, axis, )$
		\State $mid = partition(split, axis, x)$
		\newline
		
		\State $\vec{upperChild} = \vec{upper}$
		\State $upperChild_{axis} = cut$
		\State $\vec{lowerChild} = \vec{lower}$
		\State $lowerChild_{axis} = cut$
		\State $d_{i \times 2 } = \left \lceil\frac{d_{i}}{2} \right \rceil$
		\State \Call{orb}{$\vec{lower}, \vec{upperChild}, x, $}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Let not apply the algorithm to our running example:

\TODO{Insert step by step algorithm here}

\subsection{Binary Search}


%Finding the median of a large particle array is an essential step for ORB. We will thus explore different algorithms alongside their advantages and disadvantages for our specific use case.
%Note that there exists approximation algorithms for example the median of medians algorithms. But the correctness guarantee of the median lying between 30\% and 70\% is not good enough in our case. If we were to implement such an approximation algorithms we would run into similar issues as described in \ref{sec:balancing}. But in this case the unequal memory balancing do not have the advantage of equal workload balancing, which in turn will worsen the performance and the maximum number of workable particles. Thus we will only consider approximation algorithms, if their approximation to the ideal values are very exact and can be determined.

 The binary search algorithm takes an array of particles, an axis, a left and right boundary as well as a percentage $r$ of particles which should be on the left side to the determined cut. Its goal is to return a position, such that the particles on the left side of the cut are equal to the percentage input parameter multiplied by all particles.
 
 Due to time constraints we will not investigate the use of approximate algorithms and leave this open to further work.

\subsubsection{Basic Binary Search}

The first step is to make an estimation for a cut. In our case we simply assume it to be the exact middle of the domain boundaries. In principle the binary cut counts the number of particles on the left side of the of an initial cut. In case the particles are less than half of all particles, it clear that the cut was too far to the left, and the new right boundary of the domain is the cut. In case the particles are more than half, we set the the left boundary as the cut. This is repeated until we find a precise cut position.




\begin{algorithm}[H]
	\caption{Basic Binary search}\label{algo:cut}
	\begin{algorithmic}[1]
		\Procedure{cut}{$x, N_j, left, right, axis, percentage$}
		\State $nLeft = 0$
		\newline
		\While{$abs(nLeft - N_j * percentage) > 0 $}
		\State $cut = (right + left ) / 2 $
		\State $nLeft\gets sum(x_{:,axis} < split)$
		\newline
		\If{$nLeft <= N_j * percentage$}
		\State $left = cut$
		\Else 
		\State $right = cut$
		\EndIf
		\newline
		\EndWhile\label{euclidendwhile}
		\State \Return $cut$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

We analyse the runtime of the cut algorithm as follows: Inside the loop we count the number of particles on the left side of the cut and since the array is unordered, we need to sweep over elements once which results in a runtime of $O(N)$. Next we determine the maximum number of iterations our loop needs to repeated. With each iteration we cut in half the domain size.  \TODO{ Thus after the first iteration the domain size is $2^{31}$. It is therefore where easy to verify that a maximum of 32 repetitions can be performed before our domain reaches a size of $2^0$. In this case the cut cannot be improved as it has already reached the maximum precision. We conclude a worst case performance of $O(N \times 32)$. }

\subsubsection{Improved Binary Search}

In this version we will replace the while loop by a for loop, furthermore we will add an early stopping condition. We will show that this change (1) improves the runtime and (2) is actually necessary in some cases.

\begin{algorithm}[H]
	\caption{Deterministic find cut with early stopping}\label{algo:cut}
	\begin{algorithmic}[1]
		\Procedure{cut}{$x, N_j, left, right, axis, percentage$}
		\State $nLeft = 0$
		\newline
		\For{$k \in {0,..,p}$}
		\State $cut = (right + left ) / 2 $
		\State $nLeft\gets sum(x_{:,axis} < split)$
		\newline
		\If{$abs(nLeft - N_j * percentage) < \alpha $}
		\State Break
		\EndIf
		\newline

		\If{$nLeft <= N_j * percentage$}
		\State $left = cut$
		\Else 
		\State $right = cut$
		\EndIf
		\newline
		\EndFor
		\State \Return $cut$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Let us consider the following example particle distribution where $particle_1$ and $particle_2$ have identical x coordinates. 

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\draw [ dashed](5 cm, 0 cm) -- (5 cm, 3 cm);
			\cutoffdistr{10}{3}{3};
			\draw (0,0) rectangle (10, 3);
		\end{tikzpicture}
	\end{center}
\end{figure}

If we want to split this domain, there would not exists a correct split, where $nLeft$ is equal to $\frac{N}{2}$. In fact this example can be made more extreme by adding more particles which are distributed along a the line.
The $\alpha$ value defines by how many particles the final cut value can be off from the actual ideal median value. In this case we would assign $particle_0$ to the left child cell and the other particles to the right child cell. Since we can extend this example where our $\alpha$ value should be equal to $N$, we also replace the while with a for loop. Since we have already shown that the maximum number of iterations is equal to $p$, we can make use of this to avoid infinite loops. 

Considering the runtime improvements, we will show that the introduction of the $\alpha$ value brings some improvements. \TODO{Elaborate}


\begin{comment}

\subsubsection{Binary Search with improved guessing}

As we can see in \ref{algo:cut}, the initial guess of the cut is simply the center of the domain. But if we consider non uniform distributions, this initial guess can possibly rather bad. Let us consider the following example:

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\def\mean{0.1}
			\def\sigma{0.1}
			\def\pi{3.14159265359}
			\begin{axis}[xmin = 0, xmax = 1, ymin=0, ymax=5, samples=60]
				\addplot[domain = 0:1,blue] {1 / (\sigma * sqrt(2 * \pi)) * exp(-0.5 * ((x - \mean) / \sigma)^2) };
				\draw (axis cs:0.5,0) -- node[left]{$c_0$} (axis cs:0.5,5);
				\draw (axis cs:0.25,0) -- node[left]{$c_1$} (axis cs:0.25,5);
				\draw (axis cs:0.125,0) -- node[left]{$c_2$} (axis cs:0.125,5);
			\end{axis}
		\end{tikzpicture}
	\end{center}
	\caption{3 iterations of binary search with naive initial guess}\label{euclid}
\end{figure}

The underlying distribution is a normal distribution. \TODO{Add reference to section about data}. We can see that the algorithm will have to sweep 3 times over the entire array to get into the region of the actual median. One suggestion for improvement is to use an approximated median as a guess for the first split. We can sample 100 particles and find their median in constant time. Then we will run the binary cut algorithm using the approximated median as an initial guess. This can reduce the runtime by a few iterations, however we cannot make any assessment about the runtime, as this improvement will make the algorithm even slower on some distributions. Let us for example consider the uniform distribution, in this case the addition is pure overhead as the center is already the most accurate initial guess.
\end{comment}



\subsection{Partition Algorithm}

We have already introduced all basic building blocks of the ORB algorithm. The only and final piece which is missing is the partition algorithm. Note that it is not necessarily part of the ORB algorithm, but rather the bookkeeping strategy of the data-structure. We want to continuously update the particles array and in the words used before, have a direct correlation between $x_i$ and $i$. This allows us to group all particles, which are contained in a cell in a fixed range of indexes of the particles array. 

When thinking about this in terms of the recursive algorithm, maintaining this data structure is pretty straight forward. Each time we split a cell into two children, we make sure that particles contained in the left child cell are found in a designated range of the particles array, and the exact same concept is applied to the right cell. The partition algorithm is equivalent to the common hoar partition algorithm. \TODO{Add reference}

\begin{algorithm}[H]
	\caption{Partition algorithm}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{partition}{$x, N_j, cut, axis$}
		\State $i = 0$
		
		\For{$k \in {0,..N_j - 1}$}
		\If{$\vec{x}_{k, axis} < split$}
		\State $i = i + 1$
		\State $x_{i}, x_{k} = x_{k}, x_{i}$
		\EndIf
		\EndFor
		
		\State $ x_{i}, x_{N_j-1} = x_{N_j-1}, x_{i}$
		\State \Return $i$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

The partition algorithm has a clearly observable runtime of $O(N)$ since we iterate over all particles once. Since we need to touch each element at least once to reshuffle the entire array there is no better algorithm than this.


\subsection{Parallelize ORB }

In the context of parallelization schemas we assume a computing model, where the number of processors is $np$. When the goal of the algorithm is workload balancing, we will want to set $d = np$ to end up with exactly as many leaf cells as we have processors. In the context of a supercomputing system $np$ could also be set no the number of nodes in the system, but the algorithms and also the messaging interfaces are usually the same. 
When we have a cell for each processor, we can distribute the workload equally in the system and avoid idle time.

Initially we assume that each processor has a subset of all the particles stored in its memory. If we apply this concept to the particles from our numeric example, we have a distribution as depicted below. Note that the particles are distributed among processor randomly.


\begin{figure}[ht]
	
	\centering
	\begin{tikzpicture}
		\begin{axis}
			[
			nodes near coords,
			xmin=-0.,
			xmax=1.,
			ymin=-0.,
			ymax=1.,
			title=Recursion depth 0,
			legend cell align=left
			]
			
			\addplot    +[
			only marks,
			point meta=explicit symbolic,
			restrict expr to domain={\thisrow{o1}}{0:0}, 
			] table [
			x=x, 
			y=y, 
			meta=id, 
			col sep=comma] 
			{particles10.csv};
			
			\addplot    +[
			only marks,
			point meta=explicit symbolic,
			restrict expr to domain={\thisrow{o1}}{1:1}, 
			] table [
			x=x, 
			y=y, 
			meta=id, 
			col sep=comma] 
			{particles10.csv};
			
			\addplot    +[
			only marks,
			point meta=explicit symbolic,
			restrict expr to domain={\thisrow{o1}}{2:2}, 
			] table [
			x=x, 
			y=y, 
			meta=id, 
			col sep=comma] 
			{particles10.csv};
			
			\legend{$p0$,$p1$,$p3$}
			
			%\draw [fill=red!20](0.0,0.0) rectangle (1, 1);
			%\node[below] at (0.5, 1){$cell_1$};
		\end{axis}
	\end{tikzpicture}
	
\end{figure}

\subsubsection{Local Partitioning}

When performing a local reshuffle, each processor sorts its particles according to the cell information available. The cell information is however being held by a single processor. This processor supervises the ORB algorithm and broadcast the data to the workers. The distribution of the particles among the processors does not change when performing a local reshuffle.

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			
			\timeline{6}{16}{3}
			
			
			\parallelloop{-2}{15.5}{8}{0.5}{loop till all cells found};
			
			\parallelloop{-1}{11.5}{7}{3.5}{loop till cut found};
			
			\communication{Broadcast cells}{0}{7}{15};
			
			\process{reorder}{0}{14};
			\process{reorder}{2}{14};
			\process{reorder}{4}{14};
			\process{reorder}{6}{14};
			
			\process{compute\\ cut}{0}{11};
			
			
			\communication{Broadcast cut from operative}{0}{7}{8};
			
			\process{count}{0}{7};
			\process{count}{2}{7};
			\process{count}{4}{7};
			\process{count}{6}{7};
			
			
			\communication{Reduce count to operative}{0}{7}{4};
			
			\process{generate\\ new \\ cells}{0}{3};
			
		\end{tikzpicture}
	\end{center}
	\caption{Parallelized ORB for np = 4 and local reshuffling}
	\label{fig:orb_parallel}
\end{figure}

\subsubsection{Global Partitioning}

In the case of global partitioning, we partition the particle in such a way, that we can decouple the left and right child's computations completely from each other. In order to achieve this, the particles need to be redistributed among threads, s.t. each thread contains only particles contained in a single cell. Which in turn means it only need to consider particles in a designated volume. Finally this allows us to run the thread completely independent from others and remove the overhead introduced by synchronization operations.
When looking at the numeric example this might look as follows:


\begin{figure}[ht]
	
	\centering
	\begin{tikzpicture}
		\begin{axis}
			[
			nodes near coords,
			xmin=-0.,
			xmax=1.,
			ymin=-0.,
			ymax=1.,
			title=Recursion depth 0,
			legend cell align=left
			]
			
			\addplot    +[
			only marks,
			point meta=explicit symbolic,
			restrict expr to domain={\thisrow{o1}}{0:0}, 
			] table [
			x=x, 
			y=y, 
			meta=id, 
			col sep=comma] 
			{particles10.csv};
			
			\addplot    +[
			only marks,
			point meta=explicit symbolic,
			restrict expr to domain={\thisrow{o1}}{1:1}, 
			] table [
			x=x, 
			y=y, 
			meta=id, 
			col sep=comma] 
			{particles10.csv};
			
			\addplot    +[
			only marks,
			point meta=explicit symbolic,
			restrict expr to domain={\thisrow{o1}}{2:2}, 
			] table [
			x=x, 
			y=y, 
			meta=id, 
			col sep=comma] 
			{particles10.csv};
			
			\legend{$p0$,$p1$,$p2$}
			
			\draw [fill=red!20](0.0,0.0) rectangle (1, 1);
			\node[above] at (0.5, 0.0){$cell_1$};
		\end{axis}
	\end{tikzpicture}
	
\end{figure}


\begin{figure}[ht]
	
	\centering
	\begin{tikzpicture}
		\begin{axis}
			[
			nodes near coords,
			xmin=-0.,
			xmax=1.,
			ymin=-0.,
			ymax=1.,
			title=Recursion depth 1,
			legend cell align=left
			]
			
			\addplot    +[
			only marks,
			point meta=explicit symbolic,
			restrict expr to domain={\thisrow{o2}}{0:0}, 
			] table [
			x=x, 
			y=y, 
			meta=id, 
			col sep=comma] 
			{particles10.csv};
			
			\addplot    +[
			only marks,
			point meta=explicit symbolic,
			restrict expr to domain={\thisrow{o2}}{1:1}, 
			] table [
			x=x, 
			y=y, 
			meta=id, 
			col sep=comma] 
			{particles10.csv};
			
			\addplot    +[
			only marks,
			point meta=explicit symbolic,
			restrict expr to domain={\thisrow{o2}}{2:2}, 
			] table [
			x=x, 
			y=y, 
			meta=id, 
			col sep=comma] 
			{particles10.csv};
			
			\legend{$p0$,$p1$,$p2$}
			
			\draw [fill=green!20](0.0,0.0) rectangle (0.65, 1);
			\draw [fill=blue!20](0.65,0.0) rectangle (1, 1);
			\node[above] at (0.3, 0){$cell_2$};
			\node[above] at (0.8, 0){$cell_3$};
		\end{axis}
	\end{tikzpicture}
	
\end{figure}

We can observe how the ownership of particle 2 and particle 1 has to be changed, in order to make sure, that all particles from $cell_3$ are owned by the same thread. This change of ownership can be done using any desired multiprocessor communication strategy, in a multi node system we off course need to rely on internode communication systems. After a succesfull ownership swap, $p_2$ can perform further steps of the orb algorithm completely independent from $p_0$ and $p_1$. This can improve performance, because due to decoupling there needs to be less synchronization overhead. On the other hand, global partitioning comes with some considerable communication costs. 

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			
			\timeline{6}{8}{3}
			
			\communication{Compute cut}{0}{7}{7};
			\communication{Generate and Broadcast new cells}{0}{7}{6};
			
			\communication{Global reshuffle}{0}{7}{5};
			
			\communication{Compute cut}{0}{3}{4};
			\communication{Generate and \\ Broadcast new cells}{0}{3}{3};
			\communication{Global reshuffle}{0}{3}{2};
			
			\communication{Compute cut}{4}{7}{4};
			\communication{Generate and \\ Broadcast new cells}{4}{7}{3};
			\communication{Global reshuffle}{4}{7}{2};
			
			
		\end{tikzpicture}
	\end{center}
	\caption{Parallelized ORB for np = 4 and global reshuffling}
	\label{fig:orb_parallel}
\end{figure}



\newpage
\section{Theoretical Analysis of ORB runtime}

\TODO{More like assumption and predictions to narrow the search space. Its more like one influences the other. The theoretical model can set limits, in terms of how fast "could you" do it. }

The nature of the ORB algorithm and its application with very large data-sizes, requires that we consider hardware specific performance metrics for concrete implementation details. Since the decision tree for the implementation details of the algorithm is vast and each implementation involves a significant amount of development time, we will further guide our path using a theoretical model. Furthermore the model should be seen as a theoretical maximum in terms of optimization. Of course this assumes a perfect CPU and perfect GPU implementation, which are both not achievable due to a lack of knowledge and a lock of time to optimize each an every single part of the code. The lack of knowledge, is at this stage mostly present in terms of GPU implementations. Still to this day, there are new papers published which discuss new implementation details of fairly fundamental algorithms. 

When creating such a model, the performance is either bound by the number of floating point operations per minute, or in some cases its bound by the memory access speed. In our case, we have the hypothesis that binary cut, as well as the partition method in ORB are bound by memory access times and not the actual processor performance.

Now how can we verify, that the most performance intensive parts of ORB are are in fact memory bound? A very straight forward method is to compare the hardware bandwidth limits in terms of bytes between the CPU and its memory with the number of operations we perform on each loaded elements and correspondingly the maximum number of flops which can be performed on the respective hardware.


\subsection{General Memory Model}

In order to have a clear terminology and understanding of a computing system, we will briefly describe a general model of computing system, which can be applied to most modern high performance systems.

A computing system consists of several nodes, where each node has one or more CPU`s where some are equipped with one or more GPU`s. Both the CPU and the GPU have their own memory which are connected by a data link. The bandwidth of this link is called Memory Bandwidth. We name the capacity of the CPU Memory Bandwidth $B_{CPU}$ and the GPU Memory Bandwidth $B_{GPU}$. Furthermore we have a separate data link between the CPU and the GPU memory which is in some cases called PCI express, NVLink (for modern NVidia GPU`s) among others. We will refer to it as $I_{GC}$. In systems with multiple CPU there is a link between the individiual CPU which we denote as $I_{CC}$. Finally we denote the link between GPUs as $I_{GG}$.

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (cpu1) at (0cm,1.5cm) {CPU1};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (cpu2) at (0cm,-1.5cm) {CPU2};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (cpum1) at (-3cm,1.5cm) {CPU1 \\ Memory};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (cpum2) at (-3cm,-1.5cm) {CPU2 \\ Memory};
			
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpu1) at (4cm,4cm) {GPU1};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpu2) at (4cm,1cm) {GPU2};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpu3) at (4cm,-1cm) {GPU3};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpu4) at (4cm,-4cm) {GPU4};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpum1) at (7cm,4cm) {GPU1 \\ Memory};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpum2) at (7cm,1cm) {GPU2 \\ Memory};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpum3) at (7cm,-1cm) {GPU3 \\ Memory};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpum4) at (7cm,-4cm) {GPU4 \\ Memory};
			
			
			\draw[red,  thick, stealth-stealth] (cpum1) to (cpu1);
			\draw[red,  thick, stealth-stealth] (cpum2) to (cpu2);
			\draw[red, double,  thick, stealth-stealth] (cpu1) to (cpu2);
			
			
			\draw[orange, dashed,  thick, stealth-stealth] (cpu1) to (gpu1);
			\draw[orange, dashed,  thick, stealth-stealth] (cpu1) to (gpu2);
			\draw[orange, dashed,  thick, stealth-stealth] (cpu2) to (gpu3);
			\draw[orange, dashed,  thick, stealth-stealth] (cpu2) to (gpu4);
			
			\draw[cyan,  thick, stealth-stealth] (gpum1) to (gpu1);
			\draw[cyan,  thick, stealth-stealth] (gpum2) to (gpu2);
			\draw[cyan,  thick, stealth-stealth] (gpum3) to (gpu3);
			\draw[cyan,  thick, stealth-stealth] (gpum4) to (gpu4);
			
			\draw[cyan, double,  thick, stealth-stealth] (gpu1) to (gpu2);
			\draw[cyan, double,  thick, stealth-stealth] (gpu3) to (gpu4);
			
			
			\node(al) at (-2cm, -3.5cm) {};
			\node (ar) at (-3cm, -3.5cm) {};
			\node[align=left] () at (0.5cm, -3.5cm) {CPU Mem Bandwidth, $B_{C}$};
			\draw[red,  thick, stealth-stealth] (al) to (ar);
			
			\node(al) at (-2cm, -4cm) {};
			\node (ar) at (-3cm, -4cm) {};
			\node[align=left] () at (0.5cm, -4cm) {GPU Mem Bandwidth, $B_{G}$};
			\draw[cyan,  thick, stealth-stealth] (al) to (ar);
			
			\node (bl) at (-2cm, -4.5cm) {};
			\node (br) at (-3cm, -4.5cm) {};
			\node[align=left] () at (0.5cm, -4.5cm) { CPU-CPU Interconnect, $I_{CC}$};
			\draw[red, double,  thick, stealth-stealth] (bl) to (br);
			
			\node (cl) at (-2cm, -5cm) {};
			\node (cr) at (-3cm, -5cm) {};
			\node[align=left] () at (0.5cm, -5cm) { GPU-CPU Interconnect, $I_{GC}$};
			\draw[orange, dashed,  thick, stealth-stealth] (cl) to (cr);
			
			\node (dl) at (-2cm, -5.5cm) {};
			\node (dr) at (-3cm, -5.5cm) {};
			\node[align=left] () at (0.5cm, -5.5cm) { GPU-GPU Interconnect, $I_{GG}$};
			\draw[cyan, double,  thick, stealth-stealth] (dl) to (dr);
			
			
		\end{tikzpicture}
	\end{center}
\end{figure}

\subsection{Datapoints of Supercomputers}


In this section we introduce a table which lists all important data points for Piz Daint, Summit and Alps. We have choosen Piz Daint because we have the possibility to test the Code on its systems, Eiger because we can also access its systems and it will give us a good reference values, due to non hybrid architecture, meaning the nodes to not have GPU. Finally we analyse its performance on Summit, as its at the time of writing this thesis, one of the most capable supercomputers in the world.

\small
\begin{figure}[H]
	\begin{center}
		\resizebox{\textwidth}{!}{%
		\begin{tabular}{|@{} c | c | c | c | }
			Constant & Piz Daint \cite{piz_daint} & Summit\cite{summit} & Alps (Eiger) \\ 
			\hline
			\# Nodes & 5704 & 4608 & 1024\\
			\# CPUs & 1 & 2 & 2\\
			CPU Model & Intel E5-2690 v3 \cite{E5-2690} & IBM POWER9 & AMD EPYC 7742\cite{AMDEPYC} \\
			CPU Mem. & 64 GB & 256 GB & ?? \\   
			$B_C$  & 68 GB/s & 170 GB/s & 204.8 GB/s x 2	\\
			$I_{CC}$ & - & 64 GB/s & ?? \\
			Base $GHZ_C$ & 2.9 GHZ & 4 GHZ & 2.25 GHZ\\
			Max $GHZ_C$ & 3.8 GHZ & 4 GHZ & 3.4 GHZ\\
			\# Cores & 12 & 22 & 64 \\
			Architexture & Haswell & & AMD Infinity Architecture \\
			Max AVX & AVX2 & ? & AVX2 \\ 
			\# GPUs & 1 & 6 & 0 \\
			GPU Model & NVIDIA P100 \cite{TESLAP100} & NVIDIA V100s \cite{NVIDIAV100} & - \\
			GPU Mem. Cap. & 16 GB & 16 GB $\times$ 6 & -\\
			$B_G$ & 732 GB/s & 900 GB/s $\times$ 6 & -\\
			$I_{GC}$ & 32 GB/s & 50 GB/s $\times$ 6 & -\\
			$I_{GG}$ & - & 50 GB/s & -\\
			Tflops & 10.6 & 14 &\\
			\# CUDA Cores & 3584 & 5120 & \\
		\end{tabular}}	
	\end{center}
	\caption{Datapoints of Supercomputers}
	\label{fig:datapoints}
\end{figure}


\subsection{Model parameters}

\begin{comment}
If we investigate the operations which are needed for the binary cut algorithm, we notice that the actual operations completed inside the loop, are only very few. When comparing the CPU implementation with GPU one, we need to not only consider benefits from increased parallelism in GPU hardware, which will lead to a higher tflops, but also possible speed-ups reached by higher bandwidths in GPU memory than CPU memory. On the other hand we need to mitigate the very low bandwidths between the GPU and CPU, which cannot be avoided altogether, as the data needs to be sent from the CPU to GPU before we can process it on the graphics processor.
\end{comment}

In order to estimate the runtime of the algorithms we denote the number of particles as $N$. Furthermore we assume a precision $p$ of 32, which is general standard for both integers and floats, furthermore its a sensible assumption for astrophysical simulations. We only look at $x$ for the runtime analysis, where it follows that the total storage of all particles is $32 \times 3 \times N bits = 4 \times 3 \times N Bytes = 12 \times N Bytes$. Furthermore we assume $d = 1024$ and $N=10^9$.


\subsection{Roofline Performance Model}

We test the our hypothesis, that ORB is, for the most part, memory bandwidth bound using the roofline model. In the roofline model, we compare the operational density, which is measured in FLOPS / per byte against the number number of flops per minute. 

For modern hardware, its fairly uncommon to release a peak flops value. Since 2008 Intel has introduced AVX (Advanced Vector Extensions), tflops benchmarks largely depend upon the compiled assembly instructions. AVX enables the processing of several floating point precision (32 bit) numbers simultaneously. The first version of AVX , enabled a 4 such operations, with AVX2 this was extended to 8 and finally AVX512 pushed the limit to 16 operations. AVX furthermore has the capability to process $c = a + b$ operations, where as some older SIMD instructions were only able to process $a = a + b$ type operations. 

Most architecture models are able to complete add AVX instructions with a CPI (cycles per instructions) of 2. For our equation we assume a perfect parallelization, meaning that gflops linearly scale with the number of processors. 

We define in equation \ref{eq:avx}, a function to estimate the number of gigaflops for a given hardware. We define GHZ as the gigahertz which can be reached by the CPU, meanwhile $NF$ is the number of floats which can be processed simultaneously using AVX. $CPI$ is as mentioned the cycles per instructions for the AVX instruction set. Finally we have $np$ which is the number of processors.

\begin{center}
	\begin{equation}
		gflops = GHZ * NF * CPI * np
	\end{equation}
\label{eq:avx}
\end{center}

In terms of arithmetic intensity, we take a look at the code for the conditional reduction of the binary cut algorithm. We ignore other parts of the code for now, as optimizing this part is one of the main targets for the thesis. The part where we count the number of particles which are to the left of a cut position, essentially comes down to this loop:

\begin{lstlisting}[language=c++]
	for(auto p= startPtr; p<endPtr; ++p) nLeft += *p < cut;
\end{lstlisting}

Where $p$ is a C-style array which stores the particles position and $nLeft$ gives the number of particles which are left of the $cut$. In terms of arithmetic intensity, we have 2 operations, a comparison and and addition. Since a single float is stored with 4 bytes, we have 0.5 operations per byte. 


\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				height=7cm,width=10cm, 
				xlabel={FLOPS / byte},
				ylabel={GFLOPS},
				xmin = 0.1, xmax = 10, ymin= 0.25, ymax=400,
				legend style={at={(0.6,0.3)},anchor=west}]
				
		
				\addplot+[name path = A, domain = 0.25:64, mark=none] {
					3.8 * 8 * 2
				};	
				
				\addlegendentryexpanded{3.8 GHZ \& AVX2 \& np=1};
				
				\addplot+[name path = A, domain = 0.25:64, mark=none] {
					3.8 * 8 * 2 * 4
				};	
				
				
				\addlegendentryexpanded{3.8 GHZ \& AVX2 \& np=4};
				
				\addplot+[name path = A, domain = 0.25:64, mark=none] {
					68 * \x
				};	
				
				\addlegendentryexpanded{68 GB/s};
				
			\end{axis}
		\end{tikzpicture}
	\end{center}
	\caption{Roofline Model for Piz Daint}
	\label{fig:modelcpu}
\end{figure}



\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				height=7cm,width=10cm, 
				xlabel={FLOPS / byte},
				ylabel={GFLOPS},
				xmin = 0.1, xmax = 10, ymin= 0.25, ymax=400,
				legend style={at={(0.6,0.3)},anchor=west}]
				
				
				\addplot+[name path = A, domain = 0.25:64, mark=none] {
					4 * 8 * 2
				};	
				
				\addlegendentryexpanded{4 GHZ \& AVX2 \& np=1};
				
				\addplot+[name path = A, domain = 0.25:64, mark=none] {
					4 * 8 * 2 * 4
				};	
				
				
				\addlegendentryexpanded{4 GHZ \& AVX2 \& np=4};
				
				\addplot+[name path = A, domain = 0.25:64, mark=none] {
					170 * \x
				};	
				
				\addlegendentryexpanded{170 GB/s};
				
			\end{axis}
		\end{tikzpicture}
	\end{center}
	\caption{Roofline Model for Piz Daint}
	\label{fig:modelcpu}
\end{figure}

\def\N{1}
\def\Bytes{12}
\def\s{12}
\def\d{8096}
\def\p{32}
In figure \ref{fig:modelcpu} We can 

We consider a minimal C++ to make measurements which can prove or disprove our assumption. We use the -S flag along with the g++ compiler to generate assembly code from the c++ source code. In this case startPtr is a pointer which indicated the first address where we want to start out iteration process. Consequently endPtr is the memory of the last entry we want to access. For testing purposes all entries in the array are set to random values between 0 and 1, and cut is set to 0.5. 

\begin{lstlisting}
for(auto p= startPtr; p<endPtr; ++p) nLeft += *p < cut;
\end{lstlisting}

In a first test, we do enable AVX, but turn on $O3$. The generated assembly code looks as follows:

\begin{lstlisting}
.L18:
	movups	(%rax), %xmm0
	addq	$16, %rax
	cmpltps	%xmm2, %xmm0
	psubd	%xmm0, %xmm1
	cmpq	%rdx, %rax
	jne	.L18
	movdqa	%xmm1, %xmm0
	movq	%rcx, %rax
	psrldq	$8, %xmm0
	andq	$-4, %rax
	paddd	%xmm0, %xmm1
	leaq	(%rbx,%rax,4), %rbx
	movdqa	%xmm1, %xmm0
	psrldq	$4, %xmm0
	paddd	%xmm0, %xmm1
	movd	%xmm1, %r12d
	cmpq	%rax, %rcx
	je	.L15
.L16:
\end{lstlisting}

If we additionally set the compile flag -march=native we can enable AVX. This yields the following assembly instructions, where we can identify the vcmplpts and vsubd commands, which are both packaged AVX instructions. This means we have a code which can be compiled to leverage hardware in an optimal way.

\begin{lstlisting}
	
.L19:
	vmovups	(%rax), %ymm3
	addq	$32, %rax
	vcmpltps	%ymm2, %ymm3, %ymm0
	vpsubd	%ymm0, %ymm1, %ymm1
	cmpq	%rdx, %rax
	jne	.L19
	vmovdqa	%xmm1, %xmm0
	vextracti128	$0x1, %ymm1, %xmm1
	vpaddd	%xmm1, %xmm0, %xmm1
	vpsrldq	$8, %xmm1, %xmm0
	vpaddd	%xmm0, %xmm1, %xmm1
	vpsrldq	$4, %xmm1, %xmm0
	movq	%rsi, %rax
	vpaddd	%xmm0, %xmm1, %xmm1
	andq	$-8, %rax
	vmovd	%xmm1, %r12d
	leaq	(%rbx,%rax,4), %rbx
	cmpq	%rax, %rsi
	je	.L27
	vzeroupper
.L17:	

\end{lstlisting}

The following test results were achieved from a Intel(R) Core(TM) i9-10885H CPU @ 2.40GHz
processor. We perform the conditional reduction on  $2^{27}$ particles. The reduction takes 244420 microseconds. This means we have a throughput of $2^{27} / 10^{12} * 10^6 / 24420 = 0.00549 tflops$. This of course does not lie anywhere near the theoretical maximum, which even for a single processor ($np = 1$) is $2.4 * 8 * 2 * 1 / 1000 = 0.0384 tflops.$. This is a strong indication, that already with a single processor solutions, we are in the realm of bandwidth limited algorithms. This effect becomes even stronger when considering parallelism. 



\subsubsection{Naive implementation}

\begin{center}
	\begin{equation}
			log(d) \times \left ( p \times \frac{ s }{B_{C}} \right ) = t
	\end{equation}
\end{center}

\vspace{5mm}


The Equation \ref{eq:gpu} for the GPU is similar, the only difference being that we use the GPU memory bandwidth $B_{GPU}$ instead of the CPU bandwidth. Furthermore we have to consider the time it takes to send the data from the CPU to the GPU. This adds the terms size divided by CPU to GPU memory bandwidth denoted as $I_{GC}$. Finally we also need to load the data from the CPU memory to the CPU before we are able to send it. 

\begin{center}
	\begin{equation}
			log(d) \times \left ( p \times \frac{s}{B_{G}} + \frac{s}{I_{GC}}  + \frac{s}{B_{C}} \right ) = t
		\label{eq:gpu}
	\end{equation}
\end{center}

\vspace{5mm}


\subsubsection{GPU tree building}\label{gpu-tree-building}

Let us now consider an alternative way of computing binary cuts. We will build part of the tree on the GPU itself. This will allow us to reduce the very costly overheads imposed by transferring data from the CPU to the GPU. The maximum number of cuts we can perform is $p \times 3$. After p cuts we have $2^{p \times 3}$ leaf cells, and have reached the precision. Note that in this case we also need to send back the information about the built tree from the GPU to the CPU. 

\begin{center}
	\begin{equation}
		log(d) \times \left ( p \times \frac{s}{B_{G}} \right ) + \frac{s}{I_{GC}} + \frac{s}{B_{C}} = t
		\label{eq:gputree}
	\end{equation}
\end{center}


\subsubsection{Batch loading}

Finally we can also try to mitigate the overheads introduced by CPU to GPU communication by sending small batches, such that the GPU can already start processing the data, before the data transfer was completed. Lets denote the number of batches by $b$. For the sake of simplicity we only make a single cut and thus can reuse Equation \ref{eq:cpu} for the CPU version. For the GPU we now omit the term for memory access from the GPU. Because we can already start processing data in parallel when the first batch arrived and $B_{G} > I_{GC}$ holds for every modern hardware. Thus the only overhead we still get is $\frac{s \div b}{B_{G}}$ for the very last batch, but as we can just increase b in relation to N, this becomes negligible. Note that this only counts for the very first iteration, thus the only difference we have is the constant of 31 and not 32. 

\begin{center}
	\begin{equation}
		(d-1) \times p \times \frac{s}{B_{G}} + 2 \times \frac{s}{I_{GC}} = t
		\label{eq:gpubatch}
	\end{equation}
\end{center}

Due to possible overhead and no real improvement over the naive GPU version, we will omit the analysis using this method.

\subsubsection{Data compression}

Since for most computer the Interconnect bandwidth is greatly smaller than the Memory bandwidth, we could potentially also think about compressing the particle data. This would increase computational costs, but would allow us to store a higher amount of particles and reduce the cost which comes from loading the particles from memory as well as sending the data over the Interconnect.

\begin{itemize}
	\item Why do we even use floats in the first place? wouldn't integers suit better since precision is uniformly equal?
	\item Reduce transferred number of bits, sacrifice precision?
\end{itemize}

\TODO{Add plots?}


\normalfont
\subsection{Piz Daint} 

Let us plugin the values from Figure \ref{fig:datapoints} into the corresponding formulas \ref{eq:cpu}, \ref{eq:gpu}, \ref{eq:cputree} and \ref{eq:gputree}.

\subsubsection{Naive implementation}
The naive implementation yields the following speeds for the naive CPU  implementation:

\pgfmathsetmacro\cpuPiz{ln(\d) / ln(2) * (\p * \s / 68)}

\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{ \s GB }{68 GB/s} \right )  = \cpuPiz s
	\end{equation}
\end{center}


And the corresponding GPU implementation:
\pgfmathsetmacro\gpuPizN{ ln(\d) / ln(2) * (\p * \s / 732 + \s / 32 + \s / 68)}
\begin{center}
	\begin{equation}
		log(\d) \times \left ( 32 \times \frac{12 GB}{732 GB/s} + \frac{12 GB}{32 GB/s}  + \frac{12 GB}{68 GB/s} \right )=  \gpuPizN s
	\end{equation}
\end{center}

This yields in a speed-up of:
\pgfmathsetmacro\speedup{\cpuPiz / \gpuPizN}
\begin{center}
	\begin{equation}
		\frac{\cpuPiz}{\gpuPizN} = \speedup \times
	\end{equation}
\end{center}


\subsubsection{GPU Tree Building}

And the corresponding GPU implementation:
\pgfmathsetmacro\gpuPizT{ ln(\d)/ln(2) * (\p * \s / 732)  + \s / 32 + \s / 68}
And the corresponding GPU implementation:
\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{\s GB}{732 GB/s} \right ) + \times \frac{\s GB}{32 GB/s}  + \frac{\s GB}{68 GB/s} = \gpuPizT s
	\end{equation}
\end{center}

This yields in a speed-up of:
\pgfmathsetmacro\speedup{\cpuPiz / \gpuPizT}
\begin{center}
	\begin{equation}
		\frac{\cpuPiz}{\gpuPizT} = \speedup \times
	\end{equation}
\end{center}

\vspace{5mm}


\subsection{Summit}

Let us plugin the values from Figure \ref{fig:datapoints} into the corresponding formulas \ref{eq:cpu}, \ref{eq:gpu}, \ref{eq:cputree} and \ref{eq:gputree}.

\subsubsection{Naive implementation}
The naive implementation yields the following speeds for the naive CPU  implementation:

\pgfmathsetmacro\cpuSummit{ln(\d) / ln(2) * (\p * \s / (170 * 2)}
\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{ \s GB }{170 GB/s \times 2} \right ) = \cpuSummit s
	\end{equation}
\end{center}

And the corresponding GPU implementation:
\pgfmathsetmacro\gpuSummitN{ln(\d) / ln(2) * (
		(\p * \s / (900 * 6) + 
		\s / (50 * 6) + \s /(170 * 2)
	)}
\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{\s GB}{900 GB/s \times 6} + \frac{\s GB}{50 GB/s \times 6}  + \frac{\s GB}{170 GB/s \ \times 2} \right )= \gpuSummitN s
	\end{equation}
\end{center}

This yields in a speed-up of:
\pgfmathsetmacro\speedup{\cpuSummit / \gpuSummitN}
\begin{center}
	\begin{equation}
		\frac{\cpuSummit s}{\gpuSummitN s} = \speedup \times 
	\end{equation}
\end{center}


\subsubsection{GPU Tree Building}

And the corresponding GPU implementation:
\pgfmathsetmacro\gpuSummitT{ln(\d) / ln(2) * 
		(\p * \s / (900 * 6))
	 +  \s / (50 * 6) + \s /(170 * 2)}
 
\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{\s GB}{900 GB/s \times 6} \right ) + \frac{\s GB}{50 GB/s \times 2}  + \frac{\s GB}{170 GB/s \times 2} = \gpuSummitT s
	\end{equation}
\end{center}

This yields in a speed-up of:
\pgfmathsetmacro\speedup{\cpuSummit / \gpuSummitT}
\begin{center}
	\begin{equation}
		\frac{\cpuSummit s}{\gpuSummitT s} = \speedup \times
	\end{equation}
\end{center}


\vspace{5mm}


\subsection{Eiger}

\pgfmathsetmacro\cpuEiger{ln(\d) / ln(2) * (\p * \s / (204.8 * 2)}

\begin{center}
	\begin{equation}
		\log(\d) \times \p \times \frac{ \s GB }{204.8 GB/s \times 2} = \cpuEiger s
	\end{equation}
\end{center}

\subsection{Conclusion}

We conclude that the GPU version with GPU Tree building enabled yields the best speedup and also performs a lot better than the CPU version. Furthermore we can observe that the speedup is bounded by $\frac{B_{GPU}}{B_{CPU}}$/, because both the CPU and GPU implemented version are ultimately limited by the bandwidth.

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			
			\begin{axis} [ybar,height=10cm,width=13cm, 
				bar width=0.8cm,
				enlargelimits=0.2,
				symbolic x coords={Piz, Summit, Eiger},
				legend columns=1,
				legend entries={CPU, Hybrid Naive, Hybrid Tree},
				ylabel={Runtime (s) },
				x tick label style={rotate=45,anchor=east}, 
				xtick=data]
				\addplot coordinates {
					(Piz,\cpuPiz ) 
					(Summit,\cpuSummit) 
					(Eiger,\cpuEiger) 
				};
			
				\addplot coordinates {
					(Piz,\gpuPizN ) 
					(Summit,\gpuSummitN ) 
				};
			
				\addplot coordinates {
					(Piz,\gpuPizT ) 
					(Summit,\gpuSummitT  ) 
				};
			\end{axis}
			
		\end{tikzpicture}
	\end{center}

\caption{Execution times of different strategies}
\label{fig:exectimes}
\end{figure}




\begin{comment}
\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xmin = -1, xmax = 13, ymin=-1, ymax=6]
\addplot[domain = 0:12,blue] {ln(\d) / ln(2) * 
(\p * x / (900 * 6))
+  x / (50 * 6) + x /(170 * 2)};
\addplot[domain = 0:12,blue] {ln(\d * 16) / ln(2) * 
(\p * x / (900 * 6))
+  x / (50 * 6) + x /(170 * 2)};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{??}
\label{fig:exectimes}
\end{figure}

\end{comment}

\newpage
\section{Implementation}

In this section we will describe the stand-alone implementation of the accelerated ORB algorithm. We will develop this somewhat separately from the PKDGrav codebase in order to have smaller and easily understandable codebase. We will however use mdl2 from PKDGrav, which is used to distribute the workload among the processors in the system. We aim to use this platform for extensive empirical testing of different strategies and evaluate weather our the proposed method indeed outperforms the CPU version. 

We will not describe every part of the code in detail, as this would exceed the scope of the thesis. However any performance critical details and all the important data-structures are described. Furthermore, we will pay special attention to the CUDA kernels and also memory management strategies, as they are of great importance.

This implementation uses a local reshuffling method, where particles do not change their owner in terms of threads or processors. This also means, that there exists a operator thread which coordinates the ORB algorithm and keeps track of all the cells.

A brief overview of the core elements:

\begin{itemize}
	\item \textbf{particles}: The particles array has ownership over all particle objects. It has the shape of a multidimensional array, where the first three axes are occupied by positional data and the rest can be filled with meta data.
	\item \textbf{Cell}: The cell class is a structure keeping track of the fundamental cell information. In essence it is the analogue to the concept of the $cell$ which we have already introduced in section \ref{section:orb}.
	\item \textbf{Services}: The services are a set of functions, which together form the building block of the ORB algorithm. This abstraction layer allows us to dynamically distribute and collect tasks from different processors. Each service builds on top of a PST class which is introduced by mdl2.
	
\end{itemize}

For this project we will use C++ 17 along with CUDA 11.3, OpenMPI 4.1.0 and Blitz++ 1.0.2 \cite{blitzcpp}. 

\subsection{MDL}

Briefly explain how mdl exactly works. 

\subsection{Particles}

To implement the particles array, we use C-style arrays interchangeably with blitz++ arrays. The blitz++ array is a wrapper class around C-style arrays, which helps with pointer management and can speed up the debugging process by keeping track of the array size and checking access indexes. Furthermore, we can easily create slices and make borrowed copies. 
Blitz++ allows us to easily access the C-style array with the command $array.data()$, enabling us to switch to C-style arrays, whenever its deemed necessarily. 

The particles array is essentially a 2D array where each row represents a single particle. Since the performance critical part, the binary cut algorithm, of ORB only iterates over a single axis, we can further enhance the performance by choosing a column major storage format. This ensures that memory accesses are happening over consecutive data, and cache misses are limited.

We have observed cases, where iterating over a blitz array does not enable AVX, even though in theory the storage order would allow so. However iterating over the underlying C-style array did enable AVX.

\subsection{Mapping cells to particles}

Since only the operative needs to keep track of the cells, we need a separate data-structure which defines the range of particles which are encompassed inside the volume of a cell, for each thread. We solve this problem with a 2D array, where we map from a cell ID to its start index in the particles array, as well as its end index. Of course, this array needs to be updated each time we perform a reshuffling.

\subsection{Cell}

The tree which is constructed by ORB consists of multiple cells and ultimately we need to find a suitable data-structure. The most obvious choice is a tree, which keeps track of its children using pointers. But as we have seen in section \ref{section:orb-example} the conditions for a nearly complete binary tree hold, allowing for a heap to be used as storage method for the tree. The heap A heap of course has constant $O(1)$ access times over $O(log(d))$ for the tree.


\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[
			%  -{Stealth[length = 2.5pt]},
			start chain = going right,
			node distance = 0pt,
			MyStyle/.style={draw, minimum width=2em, minimum height=2em, 
				outer sep=0pt, on chain},
			]
			\node [MyStyle] (1) {$cell_1$};
			\node [MyStyle] (2) {$cell_2$};
			\node [MyStyle] (3) {$cell_3$};
			\node [MyStyle] (4) {$cell_4$};
			\node [MyStyle] (5) {$cell_5$};
			\begin{scope}[-{Stealth[length = 2.5pt]}]
				\draw (1.north) [out=25, in=155] to (2.north);
				\draw (1.north) [out=30, in=155] to (3.north);
				\draw (2.north) [out=35, in=155] to (4.north);
				\draw (2.north) [out=40, in=155] to (5.north);
			\end{scope}
		\end{tikzpicture}
		\qquad
		\begin{figure}
			\Tree[.cell_1 [.cell_2 [.cell_4 ] [.cell_5 ] ]
			[.cell_3 ]]
		\end{figure}
	\end{center}
	\caption{Tree as heap}
\end{figure}

When a heap array exceeds its allocated memory space, we need to reallocate memory, which can be very time intensive, especially for very large arrays. In our case however, its possible to set an upper boundary for the maximum number of cells. Another positive aspect of the heap is that the interface of mdl2 requires data to be sent in an array structure anyways, further decreasing performance loss and algorithmic complexity.


Since the last level of the tree, in cases when $d$ s not a power of two, is only partially filled with leaf nodes, we need to be careful when applying the algorithm. However we can derive all crucial metrics for a given tree, based on the basic heap conditions.

\subsubsection{Heap Conditions}
As with all heap data-structures we have the following conditions:

\begin{itemize}
	\item The root cell has index 1
	\item The left child of cell at index $i$ can be accessed at the index $i\times2$. 
	\item The left child of cell at index $i$ can be accessed at the index $i\times2 + 1$. 
\end{itemize}

\vspace{0.5cm}
Furthermore we can derive the following constraints given the number of leaf cells $d$:

\begin{itemize}
	\item We can compute the depth of the tree with: $\lceil log_2(d) \rceil$
	\item The number of leaf cells on the second last level is given by $2^{depth} - d$ 
	\item There are exactly $2 \times d - 2^{depth}$ items (which must all be leaf cells) on the last level. 
	\item The total number of cells are $2\times d - 1$.
\end{itemize}

\subsubsection{Class}
The cell class consists of the following variables:

\begin{itemize}
	\item $id$: The id is a unique identification of the cell instance.
	\item $nLeafCells$: this variable is equal to $d_{cell}$ and depicts the number of leaf cells which can be found when walking all possible child cells of this cell. 
	\item $lower$: The lower is 3D point coordinate which represents the lower boundary corner of the 3D volume $V_{cell}$ which is encompassed by this cells domain.
	\item $upper$: The represents the upper boundary corner of the volume.	
	\item $tmp$: The cell has room for additional data, which may be stored during executions of algorithms. 
\end{itemize}

\subsection{Operative}

Describe operative code here. Mention why we use for loop instead of recursive version. Inlcude code? -> quiet long

\subsection{Services}

As required by mdl, we make use of services, which are essentially a collection of functions which can be sent to any set of processors in the computing system. We abstract every performance critical part of the orb main runtime, which can parallelized and distributed among processors.

The services we have implemented and which are crucial for the CPU version are:

\begin{itemize}
	\item $initService$ : Reads the particles and allocates memory on the CPU accordingly.
	\item $finallizeService$ : Frees all the memory and ensures a memory leak free execution.  
	\item $countLeftService$ : Counts the number of particles left of a provided cut position for each of the cells.
	\item $countService$ : Counts the total number particles inside the domain for each of the provided cells.
	\item $localReshuffleService$ : Reshuffles the particles locally, such that their memory locality corresponds to a cell. 
\end{itemize}

\subsubsection{countLeftService}

The countLeftService takes as a input an array of cells and stores the counts in an output array. Each cell comes with a cut position and a cut axis, thus the service counts the number of particles to the left of this position in the given axis. 

As for every service, we have access to the input array with a pointer to its first element in memory called $in$ and correspondingly $out$ for the output array. We then iterate over the number of cells and access the cell structure given the cellPtrOffset as seen on line number two.
We then check weather the proper cut has already been found, if so we directly continue with the next cell (Line 4-6). We use the cellToRangeMap, which is stored as local data of the pst to retrieve the begin and end index of this cell in the particles array. (Line 7-8).
We then take a slice of the particles array and store a pointer to the data in $startPtr$ and correspondingly compute the end pointer as well. (10-14).
Finally we simple iterate over the particles and count up, as previously mentioned, this can enable AVX, given hardware support and the proper compile flags. Usually $-march=native$ along with $O3$ will enable AVX. 

\begin{lstlisting}[language=c++]
 for (int cellPtrOffset=0; cellPtrOffset<nCells; ++cellPtrOffset){
	auto cell = static_cast<Cell>(*(in + cellPtrOffset));
	
	if (cell.foundCut) {
		continue;
	}
	int beginInd = pst->lcl->cellToRangeMap(cell.id, 0);
	int endInd =  pst->lcl->cellToRangeMap(cell.id, 1);
	
	blitz::Array<float,1> particles =
		pst->lcl->particlesAxis(blitz::Range(beginInd, endInd));
	
	float * startPtr = particles.data();
	float * endPtr = startPtr + (endInd - beginInd);
	
	int nLeft = 0;
	float cut = cell.getCut();
	for(auto p= startPtr; p<endPtr; ++p)
	{
		nLeft += *p < cut;
	}
	
	out[cellPtrOffset] = nLeft;
}
\end{lstlisting}


\subsubsection{countService}

Counting the total number of particles is very trivial, given the cellToRange map. 

\begin{lstlisting}[language=c++]
for (int cellPtrOffset=0; cellPtrOffset<nCells; ++cellPtrOffset) {
	auto cell = static_cast<Cell>(*(in + cellPtrOffset));
	out[cellPtrOffset] = 
		lcl->cellToRangeMap(cell.id,1) - 
		lcl->cellToRangeMap(cell.id,0);
}

\end{lstlisting}

\subsubsection{localReshuffleService}
The local reshuffle service is responsible for rearranging the particles in such a way, that each range in the particles array, corresponds to a single cell. 
Its implementation is equivalent to a hoar partition. The implementation is straight forward and there exists no known way to improve this, at least from an algorithmic perspective. There might be some ways to explore in terms of memory access speed, or even porting the algorithm to CUDA. 
 
On line 31 till 37 we also update the cellToRangeMap in order to reflect the changes we have made in the particles array.

\begin{lstlisting}[language=c++]
for (int cellPtrOffset=0; cellPtrOffset<nCells; ++cellPtrOffset){
	auto cell = static_cast<Cell>(*(in + cellPtrOffset));
	
	int beginInd = pst->lcl->cellToRangeMap(cell.id, 0);
	int endInd = pst->lcl->cellToRangeMap(cell.id, 1);
	
	int i = beginInd-1, j = endInd;
	float cut = cell.getCut();
	
	while(true)
	{
		do
		{
			i++;
		} while(lcl->particles(i, cell.cutAxis) < cut && i <= endInd);
		
		do
		{
			j--;
		} while(lcl->particles(j, cell.cutAxis) > cut && j >= beginInd);
		
		if(i >= j) {
			break;
		}
		
		swap(lcl->particles, i, j);
	}
	
	swap(lcl->particles, i, endInd -1);
	
	lcl->cellToRangeMap(cell.getLeftChildId(), 0) =
	lcl->cellToRangeMap(cell.id, 0);
	lcl->cellToRangeMap(cell.getLeftChildId(), 1) = i;
	
	lcl->cellToRangeMap(cell.getRightChildId(), 0) = i;
	lcl->cellToRangeMap(cell.getRightChildId(), 1) =
	lcl->cellToRangeMap(cell.id, 1);
	
}
\end{lstlisting}

\subsection{CUDA Implementations}

Here we describe some first time measurements and where the actual bottlenecks are. We explain why the binary cut algorithm is the first target for GPU parallelization.
We visually explain the CPU measurements using a timeline diagram.

\begin{figure}
	\begin{center}
		\begin{tikzpicture}
			% draw horizontal line   
			\draw[thick, -Triangle] (0,0) -- (\textwidth,0) node[font=\scriptsize,below left=3pt and -8pt]{t};
			\draw[darkgray, line width=4pt] 
			(0.5,.5) -- +(1,0);
			
			\draw[red, line width=4pt] 
			(1.5,1.0) -- +(3,0);
			
			\draw[green, line width=4pt] 
			(4.5,1.5) -- +(3,0);
	
		\end{tikzpicture}
	\end{center}
	\caption{timeline}
	\label{u}
\end{figure}

As of now the time
Our primary focus for optimization is the binary cut algorithm, more specifically the counting method. Its implementation in CUDA is straight forward and performance improvement are guaranteed as shown in section \TODO{add reference}. 

\subsubsection{CUDA Reduction}
We have described a CPU implementation of the countLeftService. In this section we explain how we can port this problem to the GPU. In essence we can use a general reduction as a basis version, and implement it to specifically fit our needs, which in our case is summing up elements which fulfill a certain condition. The condition being smaller than a given value.

Optimized reductions in CUDA are well documented and explained by the NVIDIA developer team. We make use of a reduction introduced by \TODO{Reference} and adapt it slightly to fit our needs.

In a reduction we decompose the problem into smaller subproblem which can be run independently. In our case we decompose the array into smaller arrays, for each of which we apply the conditional summation. 

In the context of CUDA, we apply a decomposition and run each subproblem on a single CUDA block, this allows us to leverage fast shared memory register, keep the number of operations per thread in a reasonable range and because the subproblems can be solved independently from each other, blocks can be executed in parallel or also serially without any conflicts or synchronization. In fact, since the number of blocks which can be run concurrently on the GPU is limited, some blocks will be forced to run serially with a problem size as large as ours. \TODO{Explain cuda blocks, threads, shared memory?} After running the conditional summation on each slice of the array, we end up with a an array of results. These results can then be summed together on CPU using a simple iterator, or we can invoke another sum reduction kernel for this task.

There are several key points which need to be addressed in order make the algorithm as efficient as possible. 

\begin{enumerate}
	\item \textbf{Shared Memory} 
	In the code depicted in figure \ref{cuda:reduction} on line 3 we invocate the shared memory, where its size is dynamic as marked with the extern keyword. In a next step (lines 10-13) we apply our condition and copy the results to shared memory. This allows us to work exclusively with shared memory, reducing the amount of costly accesses performed on global memory.
	
	\item \textbf{Ops Per Thread} There exists an optimum when considering the number of operation performed by a single thread. In order to adapt the number of ops dynamically, we iterate over a certain number of elements in the global memory, as seen in figure \ref{cuda:reduction} lines 10-13. The input parameter n defines the total number of particles. Therefore if we reduce the total number of blocks by a factor of $r$, the while loop will iterate over $r$ elements. In our case we set $r$ to 32. 
	
	\item \textbf{Meta Programming} The reduction is optimized by using a template, which indicates the total number of thread per block, which is equivalent to the blockSize. Since the template is evaluated at compile time, all the if statements taking blockSize as a parameter in figure \ref{cuda:reduction} and figure \ref{cuda:warp-device} do not cause any performance loss. In fact, because we are able to unroll loops, which would be evaluated at runtime, we increase the performance of the code. 

	\item \textbf{Shared Memory Bank Conflicts} In order to decrease the bank conflicts in the shared memory we want to aim for coalesced memory access. This means there should not be any or at least no large gaps between individual read / write operations on shared memory. We can observe on lines 18, 23, 38 how we all threads write to a continuous memory segment without any gaps.
	
	\item \textbf{Warp divergence} Each time we we have an control statement in the code, we introduce a divergence. On the GPU these divergences become especially bad when they are introduced inside a warp. This means that any of the threads of the 32 threads in a warp, have to execute a different code than the rest. For this reason, the $warpReduce$ method as seen in figure \ref{cuda:warp-device} is introduced. We can see on line 32 in figure \ref{cuda:reduction} that this method is only executed when we reach 32 elements in the shared memory, which remain to be added together. Inside the warp reduce method, we again encounter an unrolled loop, but in this case we have no if statements that could generate a divergence. The if statements which are present, are as mentioned, evaluated at compile time only. Inside a warp we are also not dependent on synchronization, thus the thread synchronization call can be omitted as well. Note that this implementation will perform many unnecessary operations, but this does not affect the performance negatively.
\end{enumerate}

In our case we invoke the reduction for each cell once, where we distribute the particles within the cell among a number of blocks. The number of blocks can be determined by the number of particles in this cell $n_{cell}$ divided by the number of threads per block which is set to 512 and finally we devide this further by the number of elements per thread $r$. This of course makes the implementation straight forward but we have to pay attention to keep the number of particles large enough. As we increase the number of cells, we make more calls to the reduction kernel, where each call processes less elements. At some point a single cell may not even have enough elements left to keep a single block busy. This will results in a lot of unused warps and hinder performance. Even if we do not reach this point, as we increase the number of cells, in theory we can have an almost unoccupied block for each cell. Considering 1024 cells this is considerable and leads to a reduced warp occupancy. 

In order to resolve this, we have 

  Nevertheless it leads to reduced warp occupancy rate. However compared to earlier calls, we have few kernel calls with large array sections, the performance is greatly reduced. In fact we have observed a 10 fold reduction in speed for the binary cut algorithm. Some of the effects can also be explained by  \TODO{insert}.

Another approach is to adapt the kernel, such that it directly iterates over all the particles. But this approach cannot be implemented easily and directly comes with other drawbacks. Since we need to know the total number of elements smaller than a value for each cell, we have to be very careful about where we store results, as there should not be any 

\begin{itemize}
	\item As we increase the number of cells, we make more calls to the reduction kernel 
	
\end{itemize}

\begin{figure}[H]
	
	\begin{lstlisting}[language=c++]
template <unsigned int blockSize>
__global__ void reduce(float *g_idata, uint *g_odata, float cut, int n) {
	extern __shared__ int sdata[];
	
	unsigned int tid = threadIdx.x;
	unsigned int i = blockIdx.x + threadIdx.x;
	unsigned int gridSize = blockSize*gridDim.x;
	sdata[tid] = 0;
	
	while (i < n) {
		sdata[tid] += (g_idata[i] < cut);
		i += gridSize;
	}
	__syncthreads();
	
   	if (blockSize >= 512) {
		if (tid < 256) {
			sdata[tid] += sdata[tid + 256];
		}
	}
	if (blockSize >= 256) {
		if (tid < 128) {
			sdata[tid] += sdata[tid + 128];
		} __syncthreads();
	}
	if (blockSize >= 128) {
		if (tid < 64) {
			sdata[tid] += sdata[tid + 64];
		} __syncthreads();
	}
	if (tid < 32) {
		warpReduce<blockSize>(sdata, tid);
	}
	if (tid == 0) {
		g_odata[blockIdx.x] = sdata[0];
	}
}
	\end{lstlisting}
	\caption{Global conditional reduction kernel}
	\label{cuda:reduction}
\end{figure}

\begin{figure}[H]
	\begin{lstlisting}[language=c++]
		template <unsigned int blockSize>
		__device__ void warpReduce(volatile int *sdata, unsigned int tid) {
			if (blockSize >= 64) sdata[tid] += sdata[tid + 32];
			if (blockSize >= 32) sdata[tid] += sdata[tid + 16];
			if (blockSize >= 16) sdata[tid] += sdata[tid + 8];
			if (blockSize >= 8) sdata[tid] += sdata[tid + 4];
			if (blockSize >= 4) sdata[tid] += sdata[tid + 2];
			if (blockSize >= 2) sdata[tid] += sdata[tid + 1];
		}
		
	\end{lstlisting}
	\caption{Device warp reduction kernel}
	\label{cuda:warp-device}
\end{figure}


\subsubsection{Memory allocation}

We can use several strategies in order to improve the performance of memory allocation: 

\begin{enumerate}
	\item \textbf{Pinned Memory} As the GPU cannot access the default paged memory directly, when copying memory from the host to the device, the memory must first be copied to pinned memory. This means if we use pinned memory directly with $cudaMallocHost()$, data transfers can be around twice as fast. In our implementation we use pinned memory for all data, which need to be either sent from the host to device or vice versa.
	\item \textbf{Reuse Memory} The most obvious choice is to avoid allocation and freeing commands altogether and instead reuse previously allocated memory whenever possible. In our implementation we allocate the memory in the very beginning, whereas we have a fixed number of particles and a fixed upper limit for the number of cells. 
	
\end{enumerate}

We limit memory allocation and freeing to the very beginning and end of the orb algorithm. This reduces the runtime of the algorithm by a substantial factor. Furthermore we use pinned memory

\subsubsection{Data Transfer}

We can use several strategies in order to improve the performance of data transfers: 

\begin{enumerate}
	\item \textbf{Reuse Data} The most obvious choice is to avoid transfers altogether and and instead reuse previously sent data whenever possible. 
	\item \textbf{Reduce Data} 
	\item \textbf{CUDA Streams} 
	\item \textbf{Data Batches}
	
\end{enumerate}

Most importantly we try to reduce the overhead generated 

\subsubsection{CUDA streams}

Cuda streams can be leveraged to improve the performance of data-intensive applications by allowing different tasks to be executed in parallel on the GPU. This can be used to overlap data transfers with computation, or to execute multiple kernels concurrently. Some GPU's have several Kernel engines, this means that the execution of kernel code can be overlapped when we make use of streams.

\subsubsection{Reshuffling on the GPU}

We describe measurements which were done using the reduction in CUDA, we mention how the reduction cannot necessarily be improved a lot and that the data transfer between the GPU and CPU is the main bottleneck now. We mention how building the tree on the does not make a lot of sense. We explain why instead we focus our efforts to implement a GPU reshuffling method, which can improve runtime by reducing the reshuffling costs and also remove CPU GPU transfers between invocations of the binary cut algorithm altogether. 

We add a timeline which is based on measurements with a CUDA enhanced binary cut algorithm. 
\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			% draw horizontal line   
			\draw[thick, -Triangle] (0,0) -- (\textwidth,0) node[font=\scriptsize,below left=3pt and -8pt]{t};
			\draw[darkgray, line width=4pt] 
			(0.5,.5) -- +(1,0);
			
			\draw[red, line width=4pt] 
			(1.5,1.0) -- +(3,0);
			
			\draw[green, line width=4pt] 
			(4.5,1.5) -- +(3,0);
			
		\end{tikzpicture}
	\end{center}
	\caption{timeline}
	\label{u}
\end{figure}

We mention how the partitioning algorithm can be taken from quick sort implementations, where the most performance critical part is the scan algorithm. 
\newpage
\subsection{Integration into PKDGRAV}



\newpage
\section{Performance Analysis of ORB}

We will use two system for our performance analysis. One is the Summit supercomputer which has been described in section 3.1. The other has the following specs:


AMD EPYC 7702 64-Core Processor, Tesla T4.


\subsection{Methodology}

\subsection{Results}


\begin{comment}
	\begin{figure}[H]
		\begin{center}
			\begin{tikzpicture}
				\begin{axis}[
					height=10cm,width=13cm, 
					title={Measured Performance with $d = 1024$},
					xlabel={Particle Count},
					ylabel={Execution Time (ms)},
					]
					
					\foreach \i in {0,...,3}{
						\pgfmathsetmacro\suffix{int(pow(2,\i))};
						
						\addplot +[] 
						table [col sep=comma, x=N, y=time] 
						{../../code/out/measurements\suffix.csv};
						
						\addlegendentryexpanded{\# $\suffix$};
						
					}	
				\end{axis}
			\end{tikzpicture}
		\end{center}
	\end{figure}
\end{comment}


\subsection{Comparison to Theoretical Model}


\subsection{Conclusion}


% Use for reduction explanation https://texample.net/tikz/examples/database-decimation-process/
%\bibliographystyle{apa}
\bibliography{reference}

\end{document}
