\documentclass[]{article}

% Packages

% Pseudocode
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{arrayjob}

% Placing
\usepackage{float}
% Drawing
\usepackage{tikz}
\usetikzlibrary{arrows.meta,chains,%
	decorations.pathreplacing}
% Memory maps
\usepackage{bytefield}
% Simple trees
\usepackage{qtree}
% Custom colors
\usepackage{xcolor}
% Comments
\usepackage{verbatim}
% math symbols
\usepackage{amssymb}

% Plots
\usepackage{pgfplots}
\pgfplotsset{compat = newest}

% Dependencies
\usepackage{mymacros}
\usepackage{parallelP}


% Title
\title{Orthogonal Recursive Bisection on the GPU for Accelerated Load Balancing in Large N-Body Simulations \\ - \\ Bachelor Thesis}

\author{Andrin Rehmann}

% Start
\begin{document}

\maketitle

\newpage

\tableofcontents

\newpage
\section{Introduction}


The N-Body technique has been used for decades to simulate the Universe so we can compare theory with observations. This technique uses ``particles'' or ``bodies'' to sample phase space, and as gravity operates over infinite distance it is necessary to consider all pair-wise interactions which makes a naive implementation $\mathcal{O}(n^2)$.
It is clear that this does not scale particularly well with large particle counts.

One common approach is to decompose the particles into a tree structure and multipole expansions of the particles in each tree cell to approximate the forces. This reduces the complexity of the algorithm to $\mathcal{O}(n\log{}n)$. More recently the Fast Multipole Method (FMM) has gained wider option primarily due to the extremely large sizes of modern simulations. This technique further reduces the complexity to $\mathcal{O}(n)$!

The computational effort required in modern N-Body simulations can thus be split into three categories:

\begin{itemize}
	\item \textbf{Load Balancing:} Distribute particles equally among nodes with regards to memory.
	\item \textbf{Tree Building:} Build tree on each node for accelerated force calculation and integration.
	\item \textbf{Force calculation and integration:} Calculate forces between particles and apply them.
\end{itemize}

Before the implementation of FMM into codes, and particularly before the era of modern accelerated computing (e.g., SIMD vectorization and GPU computing) the forces calculations dominated the computational cost. In more recent simulations, each category is about one third of the total calculation time\cite{2017ComAC...4....2P}. This makes the tree building and load balancing subjects to great performance gains, since GPU acceleration is usually not exploited. 

This project proposes to implement Tree Building with the GPU using CUDA to accelerate load balancing.

\subsection{Target Data}\label{section:target-data}

\TODO{Write about properties of datasets}

The target data consists of $N$ particles where each particle has several data points and introduce the following definitions:

\begin{itemize}
	\item We define a particle as $p_i$  where $i \in \{0,...,N\}$. 
	\item We define the space of binary numbers with a precision of $p$ as $\mathbb{B}_p$ where we have $a \in \mathbb{B}_p \Leftrightarrow a \in \{0,1\}^{p}$
	\item We define the corner coordinates of root domain with $\vec{lower}, \vec{upper}$ where we have $\vec{b} \in \mathbb{B}_p^3$. 
	\item We define the coordinates of a particle $p_i$ as $\vec{x_i}$ for which holds $\{\vec{x} | \vec{lower} \leq \vec{x} \leq \vec{upper}, \vec{x} \in \mathbb{B}_p^3 \}$.

\end{itemize}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\randistr{10}{5}{100}
			\draw (0,0) rectangle (10, 5);
			\filldraw  (0,0) circle (3pt) node [anchor=west]{};
			\node[yshift=0.3cm, xshift=-0.3cm] at (0,0) {$-\frac{b}{2}$};
			
			\filldraw  (10,5) circle (3pt) node [anchor=west]{};
			\node[yshift=0.3cm, xshift=-0.3cm] at (10,5) {$\frac{b}{2}$};
		\end{tikzpicture}
	\end{center}
\caption{Unfiorm random distribution of 3D coordinates in square domain projected onto a 2d plane}
\end{figure}

\Q{Ask Doug}

\subsection{Force Integration and FFM}\label{section:force-integration}

\TODO{Write about how the very unequal data can also affect integration }
Non equal weighting functions for different particles are due to higher accelerations and greater proximity to strong gravitational influences. This in turn requires a higher integrations accuracy to mitigate errors. When we try to balance the workload, this implies drawbacks in terms of memory balance. 



\subsection{Target Computing Systems}\label{section:target-systems}

\TODO{Write about systems which can be used to test data and systems which are worth considering while estimating runtimes of this approach.}

\subsection{PKDGrav}

\TODO{Brief summary of PKDGrav and how this work can be used to improve PKDGrav}

\newpage
\section{Orthogonal Recursive Bisection (ORB)} \label{section:orb}


%https://de.overleaf.com/learn/latex/TikZ_package
%https://texample.net/tikz/examples/
In this section we will introduce the ORB algorithm along with its subroutines.
In principle the ORB Algorithm is used to partition a multi dimensional domain into subdomains based on spatial proximity. It does so by splitting a $k$ dimensional domain containing $N$ particles into $d$ subdomains each of $k$ dimensions. This is most efficiently achieved using a recursive algorithm. During the recursive process, we can store all intermediate domains a binary tree. We commonly refer to a node of this tree as $cell$. A $cell$ is a datastructure which keeps track of the domain information and also keeps pointers to its left and right child cells. When regarding the final tree, we will notice that the number of leaf nodes, or in our case leaf cells must be exactly equal to $d$. 

When we have built the binary tree successfully, we can leverage it in two ways:

\begin{enumerate}
	\item For simulation code and more specifically for the Fast Multipole Method the tree can be leveraged to greatly accelerate the simulation time. \TODO{expand}
	\item The tree can be used to equally distribute memory and/or workload among nodes or processors in a computing system. 
\end{enumerate} 

Our main objective in this work is to improve the runtime of astrophysical simulations, or more specifically PKDGrav. At the same time we want to minimize penalties in regards to $N$, the total number of particles, such that we can leverage a high $N$ to increase the precision of the simulation. Furthermore we want to adapt and optimize the algorithms with regards to the target data as introduced in \ref{section:target-data} and target computing systems (\ref{section:target-systems}).

\subsection{Memory and Workload Balancing}\label{balancing}
We have now formulated our main objectives for improving ORB, which are to improve the runtime but at the same time keeping $N$ as large as possible. In this section we will explain how the two cannot be maximized at the same time, as there will we averse effects when disregarding interactions. 

As described in section \ref{section:force-integration} if we want to mitigate the error across all particles, we need to introduce more discrete time steps for certain particles than for others. The need for non constant simulations steps across the simulation, is caused by the vastly variant forces which are exercised on different particles. It is possible that certain particles follow a straight path with an almost constant velocity, whereas others are influences by strong gravitational poles. We denote the workload for a particle $p_i$ as the weighting function $w(p_i)$. In an optimally parallelized system the workload should be very similar among computing units, thus it would make sense to distribute the particles regarding the weighting function. This in turn implies an unqeual distribution of the particles with regards to memory.  

Let us consider a simple example to illustrate the point. Given the set of particles $A = {p_1, p_2, .., p_{2N/3}}$ and respectively $B = {p_{2N/3 + 1}, p_2, .., p_{N}}$. Let us now consider the that $\forall p \in A : w(p) = 1$ and $\forall p \in B : w(p) = 2$. If we assign all particles from set $A$ to process with rank 0 and the particles from B to rank 1. It is clear that $\sum_{p\in A}^{} w(p) = \sum_{p\in B}^{} w(p)$. Thus the two processors are balanced in terms of computing costs, but clearly they are not balanced in terms of memory size. In fact process with rank 0 has $2N/3$ elements and rank 1 has $N/3$ elements. Thus there will be processes with free memory capacity which are not taken advantage of. This in turn means that we need to reduce the total number of particles $N$. 

To which degree we decide to favour memory over workload balancing is difficult to answer. But as for now we will parametrize the workload using the weighting function, which allows us to optimize and decide at a later stage. If we decide to completely ignore the workload balancing and solely focus on memory balancing, we can simply set the $w(p_i) = 1$ for all particles.


\subsection{ORB}


In each recursive call of the ORB algorithm, we are given a $cell$ along with a number of desired leaf cells $d_{cell}$, which of course corresponds to $d$ for the root cell. Furthermore each cell has information about the volume it encompasses (also refereed to its domain) stored as $V_{cell}$. We then split this $cell$, into two child cells $leftCell$ and $rightCell$. For the domains of $leftCell$ and $rightCell$ we have the following properties: 

\begin{center}
	\begin{equation}
		V_{cell} = V_{leftCell} \cup V_{rightChild}
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		V_{leftCell} \cap V_{rightChild} = \emptyset
	\end{equation}
\end{center}

How the domains of the child cells are exactly defined, will be explained in later stages of this section. Primarily we need to define the variable $d$ for the child cells.

Given a cell with we recursively define $nLeafCells$ for its children as follows:

\begin{center}
	\begin{equation}
		d_{leftCell} = \left \lceil\frac{d_{cell}}{2} \right \rceil 
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		d_{rightCell} = d_{cell} - d_{leftCell}
	\end{equation}
\end{center}

Finally we need to determine the number of particles, which should be encompassed in $V_{leftCell}$ and $V_{rigtCell}$. We define the number of particles as $N_{cell}$.

\begin{center}
	\begin{equation}
		N_{leftCell} = min \left \{ x \in \{0,...,N_{cell} \} : \sum_{i=0}^{x} w(p_i) \geq 0.5 \times \sum_{i=0}^{N_{cell}} w(p_i) \right \} 
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		N_{rightCell} = N_{cell} - N_{leftCell}
	\end{equation}
\end{center}


To simplify the visual and numeric explanation of the ORB algorithm, we assume $\forall i \in \{0,..,N\} : w(p_i) = 1$.

The ORB algorithm is initiated by considering the volume $V_{cell}$ which encompasses all particle coordinates. In each iteration we use a binary search algorithm, which we will introduce in later stages, to determine a cut $c$ position. This cut position is defined along a specified axis, which is determined by the dimension where the domain is largest. In three dimensional space we can then represent this cut as a plane and make simple checks weather a particle is right or left of this plane. 

The ideal cut plane is defined such that exactly $N_{leftChild}$ particles are located to its left side. This cut plane allows us to split the volume $V_{cell}$ in two subdomain, where the $V_{leftCell}$ is constrained by our original domain boundaries and the cut plane on its right side. The same can be done for the right cell, where the cut plane determines the left side boundaries of the volume $V_{rightCell}$. We repeat this process until we have segmented our initial domain into $d$ leaf cells.

The reason to choose the cut axis using the mentioned method, is that it reduces thin and non square shapes and converges to squares. 
In this non general case, where $d$ is a power of two, we are able to construct a binary tree wit exactly $d$ leaf nodes and a height of $log(d)$.

\subsubsection{By example} \ref{section:orb-example}

Let us consider a numerical example and explore it visually.
We consider the following $N$ = 7 particles:

\def\N{7}
\newarray\xs
\newarray\ys
\readarray{xs}{0.3&0.05&0.4&0.9&0.2&0.6&0.76}
\readarray{ys}{0.3&0.6&0.8&0.4&0.1&0.4&0.81}

\begin{center}
	\foreach \i in {0,...,\N}{
		$x_{\i} = (\xs(\i), \ys(\i))$\\
	}
\end{center}

\TODO{Add numeric example here}
\def\x{8}
\def\y{8}
\def\s{5}

\def\spx{0.45}
\def\spya{0.5}
\def\spyb{0.6}
\def\spya{0.5}
\def\spyb{0.5}
\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[scale=0.33]
			\pgfmathsetseed{2};
			%\foreach \i in {0,...,\N}{
					%\filldraw [red] (\xs(\i),\ys(\i)) circle (\s pt) node 	[anchor=west]{p};
		%	};
			\exdistr{\x}{\y}{\s};
			\draw (0,0) rectangle (\x, \y);
			\node[below] at (.5*\x,\y){$cell_1$};
		\end{tikzpicture}
		\qquad
		\begin{tikzpicture}[scale=0.33]
			\pgfmathsetseed{2};
			\exdistr{\x}{\y}{\s};
			\draw (0,0) rectangle (\x * \spx, \y);
			\node[below] at (\spx * 0.5 * \x,\y){$cell_2$};
			
			\draw (\x  * \spx, 0) rectangle (\x, \y);
			\node[below] at (0.63 *\x,\y){$cell_3$};
			
			\vspace{2mm}
			
		\end{tikzpicture}
		\qquad
		\begin{tikzpicture}[scale=0.33]
			\pgfmathsetseed{2};
			\exdistr{\x}{\y}{\s};
			\draw (0,0) rectangle (\x * \spx, \y);
			\node[below] at (\spx * 0.5 * \x,\y){$cell_4$};
			
			\draw (0,0) rectangle (\spx * \x, \y * \spyb);
			\node[below] at (.25*\x,\y * \spyb){$cell_5$};
			
			\draw (\x * \spx,0) rectangle (\x, \y);
			\node[below] at (.75*\x,\y){$cell_3$};
			
			\vspace{2mm}
			
		\end{tikzpicture}
	\end{center}
	\caption{Building a tree of depth 3 using the ORB algorithm }
\end{figure}

\begin{figure}[H]
	\Tree[.cell_1 [.cell_2 [.cell_4 ] [.cell_5 ] ]
	[.cell_3 ]]
	\caption{Tree datastructure of depth 3}
\end{figure}

Since $d_{cellLeft} - d_{cellRight}$ must always be $\leq 1$, it must hold that the tree is a balanced binary tree. Therefore we can conclude that the height of the tree is equal to $\lceil log(d) \rceil$ and the total number of nodes (not only leaf nodes) is $log(d) \times d$. Since we have a balanced tree, we also have the possibility of storing it in a heap data-structure, which can in turn greatly increase the performance, since we will not have to perform a tree walk to access individual cells in the tree.

\begin{algorithm}[H]
	\caption{The ORB main routine}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{orb}{$\vec{lower}, \vec{upper} ,x, d_i$}
		\State $\vec{size} = \vec{upper} - \vec{lower}$
		\State $axis = maxIndex(\vec{size}_0, \vec{size}_1, \vec{size}_2)$ \Comment{Get index of max value}
		\newline
		
		\State $cut = cut(x, \vec{lower}_{axis}, \vec{upper}_{axis}, axis, )$
		\State $mid = reshuffle(split, axis, x)$
		\newline
		
		\State $\vec{upperChild} = \vec{upper}$
		\State $upperChild_{axis} = cut$
		\State $\vec{lowerChild} = \vec{lower}$
		\State $lowerChild_{axis} = cut$
		\State $d_{i \times 2 } = \left \lceil\frac{d_{i}}{2} \right \rceil$
		\State \Call{orb}{$\vec{lower}, \vec{upperChild}, x, $}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}


\subsection{Binary Search}


%Finding the median of a large particle array is an essential step for ORB. We will thus explore different algorithms alongside their advantages and disadvantages for our specific use case.
%Note that there exists approximation algorithms for example the median of medians algorithms. But the correctness guarantee of the median lying between 30\% and 70\% is not good enough in our case. If we were to implement such an approximation algorithms we would run into similar issues as described in \ref{sec:balancing}. But in this case the unequal memory balancing do not have the advantage of equal workload balancing, which in turn will worsen the performance and the maximum number of workable particles. Thus we will only consider approximation algorithms, if their approximation to the ideal values are very exact and can be determined.

 The binary search algorithm takes an array of particles, an axis, a left and right boundary as well as a percentage $r$ of particles which should be on the left side to the determined cut. Its goal is to return a position, such that the particles on the left side of the cut are equal to the percentage input parameter multiplied by all particles.
 
 Due to time constraints we will not investigate the use of approximate algorithms and leave this open to further work.

\subsubsection{Basic Binary Search}

The first step is to make an estimation for a cut. In our case we simply assume it to be the exact middle of the domain boundaries. In principle the binary cut counts the number of particles on the left side of the of an initial cut. In case the particles are less than half of all particles, it clear that the cut was too far to the left, and the new right boundary of the domain is the cut. In case the particles are more than half, we set the the left boundary as the cut. This is repeated until we find a precise cut position.




\begin{algorithm}[H]
	\caption{Basic Binary search}\label{algo:cut}
	\begin{algorithmic}[1]
		\Procedure{cut}{$x, N_j, left, right, axis, percentage$}
		\State $nLeft = 0$
		\newline
		\While{$abs(nLeft - N_j * percentage) > 0 $}
		\State $cut = (right + left ) / 2 $
		\State $nLeft\gets sum(x_{:,axis} < split)$
		\newline
		\If{$nLeft <= N_j * percentage$}
		\State $left = cut$
		\Else 
		\State $right = cut$
		\EndIf
		\newline
		\EndWhile\label{euclidendwhile}
		\State \Return $cut$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

We analyse the runtime of the cut algorithm as follows: Inside the loop we count the number of particles on the left side of the cut and since the array is unordered, we need to sweep over elements once which results in a runtime of $O(N)$. Next we estimate the maximum number of iterations our loop needs to repeated. With each iteration we cut in half the domain size.  \TODO{ Thus after the first iteration the domain size is $2^{31}$. It is therefore where easy to verify that a maximum of 32 repetitions can be performed before our domain reaches a size of $2^0$. In this case the cut cannot be improved as it has already reached the maximum precision. We conclude a worst case performance of $O(N \times 32)$. }

\subsubsection{Improved Binary Search}

In this version we will replace the while loop by a for loop, furthermore we will add an early stopping condition. We will show that this change (1) improves the runtime and (2) is actually necessary in some cases.

\begin{algorithm}[H]
	\caption{Deterministic find cut with early stopping}\label{algo:cut}
	\begin{algorithmic}[1]
		\Procedure{cut}{$x, N_j, left, right, axis, percentage$}
		\State $nLeft = 0$
		\newline
		\For{$k \in {0,..,p}$}
		\State $cut = (right + left ) / 2 $
		\State $nLeft\gets sum(x_{:,axis} < split)$
		\newline
		\If{$abs(nLeft - N_j * percentage) < \alpha $}
		\State Break
		\EndIf
		\newline

		\If{$nLeft <= N_j * percentage$}
		\State $left = cut$
		\Else 
		\State $right = cut$
		\EndIf
		\newline
		\EndFor
		\State \Return $cut$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

Let us consider the following example particle distribution where $particle_1$ and $particle_2$ have identical x coordinates. 

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\draw [ dashed](5 cm, 0 cm) -- (5 cm, 3 cm);
			\cutoffdistr{10}{3}{3};
			\draw (0,0) rectangle (10, 3);
		\end{tikzpicture}
	\end{center}
\end{figure}

If we want to split this domain, there would not exists a correct split, where $nLeft$ is equal to $\frac{N}{2}$. In fact this example can be made more extreme by adding more particles which are distributed along a the line.
The $\alpha$ value defines by how many particles the final cut value can be off from the actual ideal median value. In this case we would assign $particle_0$ to the left child cell and the other particles to the right child cell. Since we can extend this example where our $\alpha$ value should be equal to $N$, we also replace the while with a for loop. Since we have already shown that the maximum number of iterations is equal to $p$, we can make use of this to avoid infinite loops. 

Considering the runtime improvements, we will show that the introduction of the $\alpha$ value brings some improvements. \TODO{Elaborate}


\begin{comment}

\subsubsection{Binary Search with improved guessing}

As we can see in \ref{algo:cut}, the initial guess of the cut is simply the center of the domain. But if we consider non uniform distributions, this initial guess can possibly rather bad. Let us consider the following example:

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\def\mean{0.1}
			\def\sigma{0.1}
			\def\pi{3.14159265359}
			\begin{axis}[xmin = 0, xmax = 1, ymin=0, ymax=5, samples=60]
				\addplot[domain = 0:1,blue] {1 / (\sigma * sqrt(2 * \pi)) * exp(-0.5 * ((x - \mean) / \sigma)^2) };
				\draw (axis cs:0.5,0) -- node[left]{$c_0$} (axis cs:0.5,5);
				\draw (axis cs:0.25,0) -- node[left]{$c_1$} (axis cs:0.25,5);
				\draw (axis cs:0.125,0) -- node[left]{$c_2$} (axis cs:0.125,5);
			\end{axis}
		\end{tikzpicture}
	\end{center}
	\caption{3 iterations of binary search with naive initial guess}\label{euclid}
\end{figure}

The underlying distribution is a normal distribution. \TODO{Add reference to section about data}. We can see that the algorithm will have to sweep 3 times over the entire array to get into the region of the actual median. One suggestion for improvement is to use an approximated median as a guess for the first split. We can sample 100 particles and find their median in constant time. Then we will run the binary cut algorithm using the approximated median as an initial guess. This can reduce the runtime by a few iterations, however we cannot make any assessment about the runtime, as this improvement will make the algorithm even slower on some distributions. Let us for example consider the uniform distribution, in this case the addition is pure overhead as the center is already the most accurate initial guess.
\end{comment}



\subsection{Reshuffling Algorithm}

We have already introduced all basic building blocks of the ORB algorithm. The only and final piece which is missing is the reshuffling algorithm. Note that it is not necessarily part of the ORB algorithm, but rather the bookkeeping strategy of the data-structure. We want to continuously update the particles array and in the words used before, have a direct correlation between $x_i$ and $i$. This allows us to group all particles, which are contained in a cell in a fixed range of indexes of the particles array. 

When thinking about this in terms of the recursive algorithm, maintaining this is pretty straight forward. Each time we split a cell into two children, we make sure that particles contained in the left child cell are found in a designated range of the particles array, and the same goes for the right cell.

\begin{algorithm}[H]
	\caption{Reshuffle algorithm}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{reshuffle}{$x, N_j, cut, axis$}
		\State $i = 0$
		
		\For{$k \in {0,..N_j - 1}$}
		\If{$\vec{x}_{k, axis} < split$}
		\State $i = i + 1$
		\State $x_{i}, x_{k} = x_{k}, x_{i}$
		\EndIf
		\EndFor
		
		\State $ x_{i}, x_{N_j-1} = x_{N_j-1}, x_{i}$
		\State \Return $i$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

The reshuffle algorithm has a clearly observable runtime of $O(N)$ since we iterate over all particles once. Since we need to touch each element at least once to reshuffle the entire array there is no better algorithm than this.


\subsection{Parallelize ORB }

In the context of panellization schemas we assume a computing model, where we have number of processors $np$. In most cases we want to set $d = np$ to end up with exactly as many leaf cells as we have processors. This allows us to assign a cell to each processor. 

When distributing the particles among our processors and executing the ORB algorithm, there exist two possibilities on how to approach this. Initially we load an array of particles on all processors. Note that if we take the union of all particles loaded onto all processors, we have the set of all particles. Also a particle is loaded exactly once by any of the processors. Usually its fair to assume that there exists little to no correlation $x_i$ and $i$. Thus the distribution of particles among two nodes may look similar to this:

\TODO{insert graphics with four colors of particles here}

After executing ORB 

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			
			\timeline{6}{16}{3}
			
			
			\parallelloop{-2}{15.5}{8}{0.5}{loop till all cells found};
			
			\parallelloop{-1}{11.5}{7}{3.5}{loop till cut found};
			
			\communication{Broadcast cells}{0}{7}{15};
			
			\process{local\\ reshuffle}{0}{14};
			\process{local\\ reshuffle}{2}{14};
			\process{local\\ reshuffle}{4}{14};
			\process{local\\ reshuffle}{6}{14};
			
			\process{compute\\ cut}{0}{11};
			
			
			\communication{Broadcast cut from operative}{0}{7}{8};
			
			\process{local \\ count}{0}{7};
			\process{local \\ count}{2}{7};
			\process{local \\ count}{4}{7};
			\process{local \\ count}{6}{7};
			
			
			\communication{Reduce count to operative}{0}{7}{4};
			
			\process{generate\\ new \\ cells}{0}{3};
			
		\end{tikzpicture}
	\end{center}
	\caption{Parallelized ORB for np = 4 and local reshuffling}
	\label{fig:orb_parallel}
\end{figure}


\subsubsection{Global Reshuffling}

Another strategy we can use, as soon as we have created two domains, rank 0 can process one domain and rank 1 can process the other domain. This has the advantage that not only the counting part, but also the costly reshuffling part of the ORB algorithm can be parallelized. Furthermore the amount of data which needs to be communicated between the different threads is smaller. This is due to the fact that as soon as we found our first cut, only half the array of particles 


\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			
			\timeline{6}{8}{3}
			
			\communication{Compute cut}{0}{7}{7};
			\communication{Generate and Broadcast new cells}{0}{7}{6};
			
			\communication{Global reshuffle}{0}{7}{5};
			
			\communication{Compute cut}{0}{3}{4};
			\communication{Generate and \\ Broadcast new cells}{0}{3}{3};
			\communication{Global reshuffle}{0}{3}{2};
			
			\communication{Compute cut}{4}{7}{4};
			\communication{Generate and \\ Broadcast new cells}{4}{7}{3};
			\communication{Global reshuffle}{4}{7}{2};
			
			
		\end{tikzpicture}
	\end{center}
	\caption{Parallelized ORB for np = 4 and global reshuffling}
	\label{fig:orb_parallel}
\end{figure}

\vspace{5mm}

\newpage
\section{Theoretical Analysis of ORB runtime}

The nature of the ORB algorithm and its application with very large data-sizes, requires that we consider hardware for concrete implementation details. Since the decision tree for the implementation details of the algorithm is vast and each implementation involves a significant amount of development time, we will propose a theoretical model to guide us in the decision process. Based on this decision, we rule out certain alternatives before the implementation. Finally we will try to verify the models using empirical data.

The algorithm is generally very heavy in memory access and the number of floating point operations is small. We thus need to not only consider benefits from increased parallelism in GPU hardware, but also possible speed-ups reached by generally higher bandwidths for GPU memory than CPU memory. On the other hand we need to mitigate the very low bandwidths between the GPU and CPU, which cannot be avoided altogether, as the data needs to be sent from the CPU to GPU before we can process it on the graphics processor.


\subsection{Datapoints of supercomputers}

We will compare the performance with the datapoints of Piz Daint, Summit and Alps which is a part of Eiger. We choose Piz Daint because we have the possibility to test the Code on its systems. We choose Eiger because we can also access its systems and it will give us a good reference values, due to it being a non hybrid supercomputer, meaning the nodes to not have GPU. Finally we analyse its performance on Summit, as its at the time of writing this thesis, one of the most capable supercomputers in the world.

\small
\begin{figure}[H]
	\begin{center}
		\begin{tabular}{|@{} c | c | c | c | }
			& Piz Daint \cite{piz_daint} & Summit\cite{summit} & Alps (Eiger) \\ 
			\hline
			\# Nodes & 5704 & 4608 & 1024\\
			\# CPUs & 1 & 2 & 2\\
			CPU Model & Intel E5-2690 v3 \cite{E5-2690} & IBM POWER9 & AMD EPYC 7742\cite{AMDEPYC} \\
			CPU Mem. & 64 GB & 256 GB & ?? \\   
			$B_C$  & 68 GB/s & 170 GB/s & 204.8 GB/s x 2	\\
			$I_{CC}$ & - & 64 GB/s & ?? \\
			Base $GHZ_C$ & 2.9 GHZ & 4 GHZ & 2.25 GHZ\\
			Max $GHZ_C$ & 3.8 GHZ & 4 GHZ & 3.4 GHZ\\
			\# Cores & 12 & 22 & 64 \\
			\# GPUs & 1 & 6 & 0 \\
			GPU Model & NVIDIA P100 \cite{TESLAP100} & NVIDIA V100s \cite{NVIDIAV100} & - \\
			GPU Mem. Cap. & 16 GB & 16 GB $\times$ 6 & -\\
			$B_G$ & 732 GB/s & 900 GB/s $\times$ 6 & -\\
			$I_{GC}$ & 32 GB/s & 50 GB/s $\times$ 6 & -\\
			$I_{GG}$ & - & 50 GB/s & -\\
			Tflops & 10.6 & 14 &\\
			\# CUDA Cores & 3584 & 5120 & \\
		\end{tabular}
	\end{center}
	\caption{Datapoints of Supercomputers}
	\label{fig:datapoints}
\end{figure}


\subsection{Roofline performance model}

In order to estimate the runtime of the algorithms we denote the number of particles as $N$. Furthermore we assume a precision $p$ of 32. We only look at $x$ for the runtime analysis, where it follows that the total storage of all particles is $32 \times 3 \times N bits = 4 \times 3 \times N Bytes = 12 \times N Bytes$. Furthermore we assume $d = 1024$ and $N=10^9$.

\def\N{1}
\def\Bytes{12}
\def\s{12}
\def\d{8096}
\def\p{32}


\begin{center}
	\begin{equation}
		log(d) \times \left ( p \times \frac{ s }{B_{C}} \right ) = t
	\end{equation}
\end{center}

\TODO{Consider having multiple CPUs}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				height=10cm,width=12cm, 
				xlabel={\# Processors},
				ylabel={Billion particles per second},
				xmin = -1, xmax = 71, ymin=-1, ymax=300,
				legend style={at={(0.03,0.8)},anchor=west}]
				
				\addplot+[domain = 1:70, mark=none] {
					4.5 * \x
				};	
				\addlegendentryexpanded{4.5 GHZ};

				\addplot+[domain = 1:70, mark=none] {
					3.5 * \x
				};	
				\addlegendentryexpanded{3.5 GHZ};
				
		
				\addplot+[domain = 1:70, mark=none] {
					2.5 * \x
				};	
				\addlegendentryexpanded{2.5 GHZ};
				
				\addplot+[mark=none] coordinates {(0,6) (70,6)};
				\addlegendentry{$B_{C}$ Intel}
				
				\addplot+[mark=none] coordinates {(0,14) (70,14)};
				\addlegendentry{$B_{C}$ IBM}
				
				\addplot+[mark=none] coordinates {(0,17) (70,17)};
				\addlegendentry{$B_{C}$ AMD}
				
				\node[label={Intel E5-2690 v3},circle,fill,inner sep=2pt] at (axis cs:12,45.6) {};
				
				\node[label={AMD EPYC},circle,fill,inner sep=2pt] at (axis cs:64,217.6) {};
				
				\node[label={IBM Power 9},circle,fill,inner sep=2pt] at (axis cs:22,88) {};
				
				

			
			\end{axis}
		\end{tikzpicture}
	\end{center}
	\caption{Operations per second for different GHZ and number of processors}
	\label{fig:modelcpu}
\end{figure}


\subsection{General Memory Model}

In order to have a clear terminology and understanding of a computing system, we will briefly describe a general computer model which can be applied to most modern high performance systems.

A computing system consists of several nodes, where each node has one or more CPU`s and some additionally have one or more GPU`s. Both the CPU and the GPU have their own memory which are connected by a data link. The bandwidth of this link is called Memory Bandwidth. We name the capacity of the CPU Memory Bandwidth $B_{CPU}$ and the GPU Memory Bandwidth $B_{GPU}$. Furthermore we have a separate data link between the CPU and the GPU memory which is in some cases called PCI express, NVLink (for modern NVidia GPU`s) among others. We will refer to it as $I_{GC}$. In systems with multiple CPU there is a link between the individiual CPU which we denote as $I_{CC}$. Finally we denote the link between GPUs as $I_{GG}$.

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (cpu1) at (0cm,1.5cm) {CPU1};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (cpu2) at (0cm,-1.5cm) {CPU2};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (cpum1) at (-3cm,1.5cm) {CPU1 \\ Memory};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (cpum2) at (-3cm,-1.5cm) {CPU2 \\ Memory};
			
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpu1) at (4cm,4cm) {GPU1};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpu2) at (4cm,1cm) {GPU2};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpu3) at (4cm,-1cm) {GPU3};
			
					\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpu4) at (4cm,-4cm) {GPU4};
			
							\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpum1) at (7cm,4cm) {GPU1 \\ Memory};
			
				\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpum2) at (7cm,1cm) {GPU2 \\ Memory};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpum3) at (7cm,-1cm) {GPU3 \\ Memory};
			
			\node[rectangle,
			draw = black,
			text = black,
			anchor = west,
			fill = white,
			align=center,
			minimum width = 2cm, 
			minimum height = 1.5cm] (gpum4) at (7cm,-4cm) {GPU4 \\ Memory};
		
			
			\draw[red,  thick, stealth-stealth] (cpum1) to (cpu1);
			\draw[red,  thick, stealth-stealth] (cpum2) to (cpu2);
			\draw[red, double,  thick, stealth-stealth] (cpu1) to (cpu2);
			
			
			\draw[orange, dashed,  thick, stealth-stealth] (cpu1) to (gpu1);
			\draw[orange, dashed,  thick, stealth-stealth] (cpu1) to (gpu2);
			\draw[orange, dashed,  thick, stealth-stealth] (cpu2) to (gpu3);
			\draw[orange, dashed,  thick, stealth-stealth] (cpu2) to (gpu4);
			
			\draw[cyan,  thick, stealth-stealth] (gpum1) to (gpu1);
			\draw[cyan,  thick, stealth-stealth] (gpum2) to (gpu2);
			\draw[cyan,  thick, stealth-stealth] (gpum3) to (gpu3);
			\draw[cyan,  thick, stealth-stealth] (gpum4) to (gpu4);
			
			\draw[cyan, double,  thick, stealth-stealth] (gpu1) to (gpu2);
			\draw[cyan, double,  thick, stealth-stealth] (gpu3) to (gpu4);
			
			
			\node(al) at (-2cm, -3.5cm) {};
			\node (ar) at (-3cm, -3.5cm) {};
			\node[align=left] () at (0.5cm, -3.5cm) {CPU Mem Bandwidth, $B_{C}$};
			\draw[red,  thick, stealth-stealth] (al) to (ar);
			
			\node(al) at (-2cm, -4cm) {};
			\node (ar) at (-3cm, -4cm) {};
			\node[align=left] () at (0.5cm, -4cm) {GPU Mem Bandwidth, $B_{G}$};
			\draw[cyan,  thick, stealth-stealth] (al) to (ar);
			
			\node (bl) at (-2cm, -4.5cm) {};
			\node (br) at (-3cm, -4.5cm) {};
			\node[align=left] () at (0.5cm, -4.5cm) { CPU-CPU Interconnect, $I_{CC}$};
			\draw[red, double,  thick, stealth-stealth] (bl) to (br);
			
			\node (cl) at (-2cm, -5cm) {};
			\node (cr) at (-3cm, -5cm) {};
			\node[align=left] () at (0.5cm, -5cm) { GPU-CPU Interconnect, $I_{GC}$};
			\draw[orange, dashed,  thick, stealth-stealth] (cl) to (cr);
			
			\node (dl) at (-2cm, -5.5cm) {};
			\node (dr) at (-3cm, -5.5cm) {};
			\node[align=left] () at (0.5cm, -5.5cm) { GPU-GPU Interconnect, $I_{GG}$};
			\draw[cyan, double,  thick, stealth-stealth] (dl) to (dr);
			
			
		\end{tikzpicture}
	\end{center}
\end{figure}

We now sketch out various implementation ideas, derive for each a formula to estimate its runtime and finally insert datapoints from our target computing systems.
We assume that the data is stored in the CPU memory initially and we define the size of the data as $s$. 

\subsection{Estimating tflops}

\TODO{Introduce AVX here}

\

\subsubsection{Naive implementation}

\begin{center}
	\begin{equation}
			log(d) \times \left ( p \times \frac{ s }{B_{C}} \right ) = t
	\end{equation}
\end{center}

\vspace{5mm}


The Equation \ref{eq:gpu} for the GPU is similar, the only difference being that we use the GPU memory bandwidth $B_{GPU}$ instead of the CPU bandwidth. Furthermore we have to consider the time it takes to send the data from the CPU to the GPU. This adds the terms size divided by CPU to GPU memory bandwidth denoted as $I_{GC}$. Finally we also need to load the data from the CPU memory to the CPU before we are able to send it. 

\begin{center}
	\begin{equation}
			log(d) \times \left ( p \times \frac{s}{B_{G}} + \frac{s}{I_{GC}}  + \frac{s}{B_{C}} \right ) = t
		\label{eq:gpu}
	\end{equation}
\end{center}

\vspace{5mm}


\subsubsection{GPU tree building}

Let us now consider an alternative way of computing binary cuts. We will build part of the tree on the GPU itself. This will allow us to reduce the very costly overheads imposed by transferring data from the CPU to the GPU. The maximum number of cuts we can perform is $p \times 3$. After p cuts we have $2^{p \times 3}$ leaf cells, and have reached the precision. Note that in this case we also need to send back the information about the built tree from the GPU to the CPU. 

\begin{center}
	\begin{equation}
		log(d) \times \left ( p \times \frac{s}{B_{G}} \right ) + \frac{s}{I_{GC}} + \frac{s}{B_{C}} = t
		\label{eq:gputree}
	\end{equation}
\end{center}


\subsubsection{Batch loading}

Finally we can also try to mitigate the overheads introduced by CPU to GPU communication by sending small batches, such that the GPU can already start processing the data, before the data transfer was completed. Lets denote the number of batches by $b$. For the sake of simplicity we only make a single cut and thus can reuse Equation \ref{eq:cpu} for the CPU version. For the GPU we now omit the term for memory access from the GPU. Because we can already start processing data in parallel when the first batch arrived and $B_{G} > I_{GC}$ holds for every modern hardware. Thus the only overhead we still get is $\frac{s \div b}{B_{G}}$ for the very last batch, but as we can just increase b in relation to N, this becomes negligible. Note that this only counts for the very first iteration, thus the only difference we have is the constant of 31 and not 32. 

\begin{center}
	\begin{equation}
		(d-1) \times p \times \frac{s}{B_{G}} + 2 \times \frac{s}{I_{GC}} = t
		\label{eq:gpubatch}
	\end{equation}
\end{center}

Due to possible overhead and no real improvement over the naive GPU version, we will omit the analysis using this method.

\subsubsection{Data compression}

Since for most computer the Interconnect bandwidth is greatly smaller than the Memory Bandwidth, we could potentially also think about compressing the particle data. This would increase computational costs, but would allow us to store a higher amount of particles and reduce the cost which comes from loading the particles from memory as well as sending the data over the Interconnect.

\begin{itemize}
	\item Why do we even use floats in the first place? wouldn't integers suit better since precision is uniformly equal?
	\item Reduce transfered number of bits, sacrifice precision?
\end{itemize}

\TODO{Add plots?}


\normalfont
\subsection{Piz Daint} 

Let us plugin the values from Figure \ref{fig:datapoints} into the corresponding formulas \ref{eq:cpu}, \ref{eq:gpu}, \ref{eq:cputree} and \ref{eq:gputree}.

\subsubsection{Naive implementation}
The naive implementation yields the following speeds for the naive CPU  implementation:

\pgfmathsetmacro\cpuPiz{ln(\d) / ln(2) * (\p * \s / 68)}

\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{ \s GB }{68 GB/s} \right )  = \cpuPiz s
	\end{equation}
\end{center}


And the corresponding GPU implementation:
\pgfmathsetmacro\gpuPizN{ ln(\d) / ln(2) * (\p * \s / 732 + \s / 32 + \s / 68)}
\begin{center}
	\begin{equation}
		log(\d) \times \left ( 32 \times \frac{12 GB}{732 GB/s} + \frac{12 GB}{32 GB/s}  + \frac{12 GB}{68 GB/s} \right )=  \gpuPizN s
	\end{equation}
\end{center}

This yields in a speed-up of:
\pgfmathsetmacro\speedup{\cpuPiz / \gpuPizN}
\begin{center}
	\begin{equation}
		\frac{\cpuPiz}{\gpuPizN} = \speedup \times
	\end{equation}
\end{center}


\subsubsection{GPU Tree Building}

And the corresponding GPU implementation:
\pgfmathsetmacro\gpuPizT{ ln(\d)/ln(2) * (\p * \s / 732)  + \s / 32 + \s / 68}
And the corresponding GPU implementation:
\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{\s GB}{732 GB/s} \right ) + \times \frac{\s GB}{32 GB/s}  + \frac{\s GB}{68 GB/s} = \gpuPizT s
	\end{equation}
\end{center}

This yields in a speed-up of:
\pgfmathsetmacro\speedup{\cpuPiz / \gpuPizT}
\begin{center}
	\begin{equation}
		\frac{\cpuPiz}{\gpuPizT} = \speedup \times
	\end{equation}
\end{center}

\vspace{5mm}


\subsection{Summit}

Let us plugin the values from Figure \ref{fig:datapoints} into the corresponding formulas \ref{eq:cpu}, \ref{eq:gpu}, \ref{eq:cputree} and \ref{eq:gputree}.

\subsubsection{Naive implementation}
The naive implementation yields the following speeds for the naive CPU  implementation:

\pgfmathsetmacro\cpuSummit{ln(\d) / ln(2) * (\p * \s / (170 * 2)}
\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{ \s GB }{170 GB/s \times 2} \right ) = \cpuSummit s
	\end{equation}
\end{center}

And the corresponding GPU implementation:
\pgfmathsetmacro\gpuSummitN{ln(\d) / ln(2) * (
		(\p * \s / (900 * 6) + 
		\s / (50 * 6) + \s /(170 * 2)
	)}
\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{\s GB}{900 GB/s \times 6} + \frac{\s GB}{50 GB/s \times 6}  + \frac{\s GB}{170 GB/s \ \times 2} \right )= \gpuSummitN s
	\end{equation}
\end{center}

This yields in a speed-up of:
\pgfmathsetmacro\speedup{\cpuSummit / \gpuSummitN}
\begin{center}
	\begin{equation}
		\frac{\cpuSummit s}{\gpuSummitN s} = \speedup \times 
	\end{equation}
\end{center}


\subsubsection{GPU Tree Building}

And the corresponding GPU implementation:
\pgfmathsetmacro\gpuSummitT{ln(\d) / ln(2) * 
		(\p * \s / (900 * 6))
	 +  \s / (50 * 6) + \s /(170 * 2)}
 
\begin{center}
	\begin{equation}
		log(\d) \times \left ( \p \times \frac{\s GB}{900 GB/s \times 6} \right ) + \frac{\s GB}{50 GB/s \times 2}  + \frac{\s GB}{170 GB/s \times 2} = \gpuSummitT s
	\end{equation}
\end{center}

This yields in a speed-up of:
\pgfmathsetmacro\speedup{\cpuSummit / \gpuSummitT}
\begin{center}
	\begin{equation}
		\frac{\cpuSummit s}{\gpuSummitT s} = \speedup \times
	\end{equation}
\end{center}


\vspace{5mm}


\subsection{Eiger}

\pgfmathsetmacro\cpuEiger{ln(\d) / ln(2) * (\p * \s / (204.8 * 2)}

\begin{center}
	\begin{equation}
		\log(\d) \times \p \times \frac{ \s GB }{204.8 GB/s \times 2} = \cpuEiger s
	\end{equation}
\end{center}

\subsection{Conclusion}

We conclude that the GPU version with GPU Tree building enabled yields the best speedup and also performs a lot better than the CPU version. Also we can observe that the speedup is bounded by $\frac{B_{GPU}}{B_{CPU}}$/ \TODO{Add reasoning}

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			
			\begin{axis} [ybar,height=10cm,width=13cm, 
				bar width=0.8cm,
				enlargelimits=0.2,
				symbolic x coords={Piz, Summit, Eiger},
				legend columns=1,
				legend entries={CPU, Hybrid Naive, Hybrid Tree},
				ylabel={Runtime (s) },
				x tick label style={rotate=45,anchor=east}, 
				xtick=data]
				\addplot coordinates {
					(Piz,\cpuPiz ) 
					(Summit,\cpuSummit) 
					(Eiger,\cpuEiger) 
				};
			
				\addplot coordinates {
					(Piz,\gpuPizN ) 
					(Summit,\gpuSummitN ) 
				};
			
				\addplot coordinates {
					(Piz,\gpuPizT ) 
					(Summit,\gpuSummitT  ) 
				};
			\end{axis}
			
		\end{tikzpicture}
	\end{center}

\caption{Execution times of different strategies}
\label{fig:exectimes}
\end{figure}




\begin{comment}
\begin{figure}
\begin{center}
\begin{tikzpicture}
\begin{axis}[xmin = -1, xmax = 13, ymin=-1, ymax=6]
\addplot[domain = 0:12,blue] {ln(\d) / ln(2) * 
(\p * x / (900 * 6))
+  x / (50 * 6) + x /(170 * 2)};
\addplot[domain = 0:12,blue] {ln(\d * 16) / ln(2) * 
(\p * x / (900 * 6))
+  x / (50 * 6) + x /(170 * 2)};
\end{axis}
\end{tikzpicture}
\end{center}
\caption{??}
\label{fig:exectimes}
\end{figure}

\end{comment}

\newpage
\section{Stand-alone Implementation}

\TODO{Give introduction as to how precise the description is and should be}

In this section we will describe the stand-alone implementation of the accelerated ORB algorithm. We will develop this separately from the PKDGrav codebase in order to have smaller and easily understandable codebase, furthermore we aim to use this platform for extensive empirical testing of different strategies.
We aim to implement the CPU version in the most efficient method as possible. Only when we achieve that, we can make a fair comparison between the CPU and GPU implementation. 

A brief overview of the core classes and function is as follows:

\begin{itemize}
	\item \textbf{particles}: The particles array has ownership over all particles. It has the shape of a multidimensional array, where the first three axes are occupied by positional data and the rest can be filled with meta data.
	\item \textbf{Cell}: The cell class is a structure keeping track of the fundamental cell information. In essence it is the analogue to the concept of the $cell$ which we have already introduced in section \ref{section:orb}.
	\item \textbf{ORB}: The orb class keeps track of the all particles and maintains a data-structure which maps between particles and cells. 
	\item \textbf{Services}: The services are a set of function, which together form the building block of the ORB algorithm. This abstraction layer allows us to dynamically distribute and collect tasks from different processors. Furthermore individual services can be interchanged by GPU enabled implementations.
	\item \textbf{Messaging}: The messaging class is responsible for building the interface between different processors, which essentially is dispatching services. Its can be implemented using a wide set of tools. In our case we will solely focus on an MPI implementation.
	
\end{itemize}

For this project we will use C++ 17 along with CUDA 11.3, OpenMPI 4.1.0 and Blitz++ 1.0.2 \cite{blitzcpp}. 

\subsection{Particles}

To implement the particles array, we use C-style arrays interchangeably with blitz++ arrays. The blitz++ array is a wrapper class around C-style arrays, which helps with pointer management and can speed up the debugging process by keeping track of the array size and checking access indexes. Furthermore, we can easily create slices and make borrowed copies. 
Blitz++ allows us to easily access the C-style array with the command $array.data()$, enabling us to 

\TODO{Add example with and without AVX in assembly}
\subsection{Cell}

As our tree consists of multiple cells, which are to be stored in a tree data-structure. As seen in section \ref{section:orb-example} the conditions for a balanced tree hold, allowing us to use a heap to store the tree. This \TODO{hint} reduces the runtime of the tree, and since we need to store the cells in an array for communication purposes anyways (\TODO{Why exactly?}), this makes a lot of sense.

\TODO{Either copy cell into buffer. You have to linearize. Add arguments with leaf cells.}

When using a parallization schema using local reshuffling we will have to send all cells on a single level using the messaging interface. Using the heap datastructure, cells which corresponds to single level in the tree, can be accessed using a simple range.

\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[
			%  -{Stealth[length = 2.5pt]},
			start chain = going right,
			node distance = 0pt,
			MyStyle/.style={draw, minimum width=2em, minimum height=2em, 
				outer sep=0pt, on chain},
			]
			\node [MyStyle] (1) {$cell_1$};
			\node [MyStyle] (2) {$cell_2$};
			\node [MyStyle] (3) {$cell_3$};
			\node [MyStyle] (4) {$cell_4$};
			\node [MyStyle] (5) {$cell_5$};
			\begin{scope}[-{Stealth[length = 2.5pt]}]
				\draw (1.north) [out=25, in=155] to (2.north);
				\draw (1.north) [out=30, in=155] to (3.north);
				\draw (2.north) [out=35, in=155] to (4.north);
				\draw (2.north) [out=40, in=155] to (5.north);
			\end{scope}
		\end{tikzpicture}
		\qquad
		\begin{figure}
			\Tree[.cell_1 [.cell_2 [.cell_4 ] [.cell_5 ] ]
			[.cell_3 ]]
		\end{figure}
	\end{center}
	\caption{Tree as heap}
\end{figure}

Note that due to the property of always assigning the larger desired number of leaf cells to the left child cell, there will never be gaps in the lowest level of the tree, i.e. all consecutive indices in the array are filled with a cell until the last element.

\subsubsection{Heap Conditions}
As with all heap data-structures we have the following conditions:

\begin{itemize}
	\item The root cell has index 1
	\item The left child of cell at index $i$ can be accessed at the index $i\times2$. 
	\item The left child of cell at index $i$ can be accessed at the index $i\times2 + 1$. 
\end{itemize}

\vspace{0.5cm}
Furthermore we can derive the following constraints given the number of leaf cells $d$:

\begin{itemize}
	\item We can compute the depth of the tree with: $\lceil log_2(d) \rceil$
	\item The number of cells on the second last level (which must all be leaf cells) is given by $2^{depth} - d$ 
	\item There are exactly $2 \times d - 2^{depth}$ items on the last level. 
	\item The total number of cells are $2\times d - 1$.
\end{itemize}

\TODO{Should I add the proofs here?}

\subsubsection{Class}
The cell class consists of the following variables:

\begin{itemize}
	\item $id$: The id is a unique identification of the cell instance.
	\item $nLeafCells$: this variable is equal to $d_{cell}$ and depicts the number of leaf cells which can be found when walking all possible child cells of this cell. 
	\item $lower$: The lower is 3D point coordinate which represents the lower boundary corner of the 3D volume $V_{cell}$ which is encompassed by this cells domain.
	\item $upper$: The represents the upper boundary corner of the volume.	
	\item $tmp$: The cell has room for additional data, which may be stored during executions of algorithms. 
\end{itemize}


The cells could be stored as binary tree or as we have already mentioned we could make use of a heap datastructure which allows for greater flexibility. Due to performance improvements, we will opt for the second variant.


\subsection{ORB}

\begin{itemize}
	\item $particles$: The cell class has ownership over the particles array.
	\item $cellToParticles$: This array maps from a $cell.id$ to a tuple with entries $a$ and $b$ where a marks the index of the first particle in the particles array which is contained in this cell. Consequently $b$ marks the index after the last particle contained in the volume of the cell. Note that this is possible due to the constant reshuffling.
\end{itemize}

The orb class comes with helper functions, which improve abstraction and understandability of the code. For example it introduces a swap function, which can swap two particles in the particles array.

\subsection{Services}

Similarly as in PKDGrav we introduce the concept of services, which essentially is a collection of functions which can be sent to any set of processors in the computing system. We abstract the ORB algorithm into such services, such that we can experiment with different workload distribution schemas and easily interchange subroutines with GPU enables ones.

The services we have implemented are:

\begin{itemize}
	\item $countLeftService$ : Counts the number of particles left of a provided cut position for each of the cells.
	\item $countServiceService$ : Counts the number particles inside the domain for each of the provided cells.
	\item $localReshuffleService$ : Reshuffles the particles locally, such that their memory locality corresponds to a cell. 
	\item $buildTreeService$ : This is the operative service, which distributes and coordinates other services. Its comparable to the ORB main routine. 
\end{itemize}

Each service function is associated with an unique ID.

With this system, a processor simply needs to wait for new services and fetch their corresponding parameters. The fetching and distribution of such services happens over the Messaging interface.

\subsection{CUDA Services}

In this section we describe the services which were implemented using CUDA. In this section we may also add some theory about how exactly CUDA works.


\subsection{Messaging}

The messsaging class has one main function called $dispatchService$. It takes a service ID and distributes the necessary data to the target processors. Furthermore it collects the results of the services and returns them to the source processor.

In this section we will take an in depth look at the MPI calls which have to be made in order to make this function work.

\newpage
\section{Empirical Analysis of ORB}

\subsection{Methodology}

\subsection{Results}


\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}
			\begin{axis}[
				height=10cm,width=13cm, 
				title={Measured Performance with $d = 1024$},
				xlabel={Particle Count},
				ylabel={Execution Time (ms)},
				]
				
				\foreach \i in {0,...,3}{
					\pgfmathsetmacro\suffix{int(pow(2,\i))};
					
					\addplot +[] 
					table [col sep=comma, x=N, y=time] 
					{../../code/out/measurements\suffix.csv};
					
					\addlegendentryexpanded{\# $\suffix$};
					
				}	
			\end{axis}
		\end{tikzpicture}
	\end{center}
\end{figure}

\subsection{Comparison to Theoretical Model}


\subsection{Conclusion}


% Use for reduction explanation https://texample.net/tikz/examples/database-decimation-process/
\bibliographystyle{apa}
\bibliography{reference}

\end{document}
