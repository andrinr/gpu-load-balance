\documentclass[]{article}

% Packages
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{float}
\usepackage{tikz}
\usepackage{bytefield}
\usepackage{qtree}
\usepackage{xcolor}

% Dependencies
\usepackage{mymacros}


% Title
\title{Notes}
\author{Andrin Rehmann}

% Start
\begin{document}

\maketitle

\section{Introduction}


The N-Body technique has been used for decades to simulate the Universe so we can compare theory with observations. This technique uses ``particles'' or ``bodies'' to sample phase space, and as gravity operates over infinite distance it is necessary to consider all pair-wise interactions which makes a naive implementation $\mathcal{O}(n^2)$.
It is clear that this does not scale particularly well with large particle counts.

One common approach is to decompose the particles into a tree structure and multipole expansions of the particles in each tree cell to approximate the forces. This reduces the complexity of the algorithm to $\mathcal{O}(n\log{}n)$. More recently the Fast Multipole Method (FMM) has gained wider option primarily due to the extremely large sizes of modern simulations. This technique further reduces the complexity to $\mathcal{O}(n)$!

The computational effort required in modern N-Body simulations can thus be split into three categories:

\begin{itemize}
	\item \textbf{Load Balancing:} Distribute particles equally among nodes with regards to memory.
	\item \textbf{Tree Building:} Build tree on each node for accelerated force calculation and integration.
	\item \textbf{Force calculation and integration:} Calculate forces between particles and apply them.
\end{itemize}

Before the implementation of FMM into codes, and particularly before the era of modern accelerated computing (e.g., SIMD vectorization and GPU computing) the forces calculations dominated the computational cost. In more recent simulations, each category is about one third of the total calculation time\cite{2017ComAC...4....2P}. This makes the tree building and load balancing subjects to great performance gains, since GPU acceleration is usually not exploited. 

This project proposes to implement ORB with the CUDA API to accelerate load balancing.

\section{ORB Algorithm}


%https://de.overleaf.com/learn/latex/TikZ_package
%https://texample.net/tikz/examples/
The ORB Algorithm can be used to partition a multi dimensional domain into subdomains based on spatial proximity. The advantage of subdividing the domain into smaller subdomains is two folds:

\begin{itemize}
	\item For simulation code and more specifically for the Fast Multipole Method these domains can be leveraged to greatly accelerate the simulation time. 
	\item The workload can be equally distributed among nodes and processors in large computing systems.
\end{itemize} 


The general idea of the ORB algorithm is to split a $k$ dimensional domain containing $N$ particles into $np$ subdomains using a recursive procedure. This can be repeated until the number of smallest subdomains is equal to the total count of processors or nodes. The final goal is an equal distribution of the workload and data, thus the subdomain sizes should be chosen such that the number of particles is equal among nodes / processors.

\subsection{Memory and Workload Balancing}
For the ORB algorithms itself, the relation between the number of processors and the workload holds steady. But for the simulation certain particles require more timesteps, thus more computational effort, than others. We denote the workload per particle as the waiting function $w(p_i)$ where $p_i \in {p_1, p_2, ..., p_N}$.

Non equal waiting functions for different particles are due to higher accelerations and greater proximity to strong gravitational influences. This in turn requires a higher accuracy to mitigate errors. When we try to balance the workload, this implies drawbacks in terms of memory balance. 

Let us consider a simple example to illustrate the point. Given the set of particles $A = {p_1, p_2, .., p_{2N/3}}$ and respectively $B = {p_{2N/3 + 1}, p_2, .., p_{N}}$. Let us now consider the that $\forall p \in A : w(p) = 1$ and $\forall p \in B : w(p) = 2$. If we assign all particles from set $A$ to process with rank 0 and the particles from B to rank 1. It is clear that $\sum_{p\in A}^{} w(p) = \sum_{p\in B}^{} w(p)$. Thus the two processors are balanced in terms of computing costs, but clearly they are not balanced in terms of memory size. In fact process with rank 0 has $2N/3$ elements and rank 1 has $N/3$ elements. Thus there will be processes there are memory slots which are not taken advantage of, which means we have a reduced total number of particles. As cosmological simulations profit from an increased number of particles, this also means that the quality of the simulation suffers when prioritizing workload balancing.

This exact reasoning is also the reason we favour algorithms which can be computed in place, over potentially more efficient algorithms, which cannot be computed in place. For example the algorithm to find a binary cut could be improved by using the mean of means algorithm (\TODO{Add reference}), but it cannot be done in place, reducing the maximum number of particle we can simulate.


If not denoted otherwise, we will assume $w(p_i) = 1$ for all particles in the simulation.

\subsection{ORB for power of twos}

Let us first make the assumption that $np$ is a power of two. In this case we can search for a cut such that 50\% of the particles are on the left side and likewise on the right side of the cut. The dimension of the cut is always the axis where the size of the domain is the largest. This ensures that we do not get very thin shapes, generally closer to a square. This in turn is very beneficial for the FMM as it has a higher precision on more squarish shapes.  The position of the cut can be found in $O(log(N))$ using a binary search. We can introduce an even tighter bound when assuming each particle is being stored as a 32 bit precision number. In this case we have reached the maximum precision after $O(32)$ iterations. After such a cut has been found, we can create two new subdomains and recursively apply the algorithm on both of them until we have reached the desired number of subdomains $np$. 
 
\vspace{5mm}

\def\x{8}
\def\y{8}

\def\spx{0.45}
\def\spya{0.5}
\def\spyb{0.6}
\begin{figure}[H]
\begin{center}
	\begin{tikzpicture}[scale=0.33]
		\pgfmathsetseed{2};
		\randistr{\x}{\y}{100};
		\draw (0,0) rectangle (\x, \y);
		\node[below] at (.5*\x,\y){$root cell$};
	\end{tikzpicture}
	\qquad
	\begin{tikzpicture}[scale=0.33]
		\pgfmathsetseed{2};
		\randistr{\x}{\y}{100};
		\draw (0,0) rectangle (\x * \spx, \y);
		\node[below] at (.25*\x,\y){$cell1$};
		
		\draw (\x  * \spx, 0) rectangle (\x, \y);
		\node[below] at (.75*\x,\y){$cell2$};
		
		\vspace{2mm}
		
	\end{tikzpicture}
	\qquad
	\begin{tikzpicture}[scale=0.33]
		\pgfmathsetseed{2};
		\randistr{\x}{\y}{100};
		\draw (0,0) rectangle (\x * \spx, \y * \spya);
		\node[below] at (.25*\x,\y * \spya){$cell2$};
		
		\draw (0,\y * \spya) rectangle (\x * \spx, \y);
		\node[below] at (.25*\x,\y){$cell3$};
		
		\draw (\x * \spx,0) rectangle (\x, \y * \spyb);
		\node[below] at (.75*\x,\y * \spyb){$cell4$};
		
		\draw (\x * \spx,\y * \spyb) rectangle (\x, \y);
		\node[below] at (.75*\x,\y){$cell5$};
		
		\vspace{2mm}
	
\end{tikzpicture}
\end{center}
\caption{Building a tree of depth 3 using the ORB algorithm }
\label{fig:balorb}
\end{figure}

For a clearer terminology, we will from now on refer to a domain as a cell. Furthermore the subdomain of a domain is the child cell of a cell, and likewise we also have the concept of a parent cell.

\vspace{5mm}

When each cell keeps track of its child cells, we have built a tree datastructure. For instance for our simple domain composition of depth 3 from Figure \ref{fig:balorb} we end up with structure as depicted in Figure \ref{fig:baltree} In this tree each cell represents a node, the root node is the cell which encompasses the entire space and particles and finally its leaves can be assigned to a processor / node. Note that the tree is not balanced, but each node has either two children or is a leaf. 

\begin{figure}[H]
\Tree[.rootcell [.cell1 [.cell2 ]
[.cell3 ]]
[.cell4 [.cell5 ]
[.cell5 ]]]
\caption{Tree datastructure of depth 3}
\label{fig:baltree}
\end{figure}

\subsection{ORB in the general case}

Let us now consider the general case where $np$ can be any integer number. 

In this case we chose the cut such that we have 50\% of particles to the left and right. But we define the number of particles on the left side by $l = \left \lceil{\frac{np}{2}}\right \rceil$. Consequently the number of particles on the right are $r = np - l$. In the next iteration of the recursion, the left child cell will set $np = l$ and the right side $np = r$. This ensures that we end up with the right number of domains.

\def\spx{0.36}
\def\spya{0.5}
\def\spyb{0.6}
\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[scale=0.33]
			\pgfmathsetseed{2};
			\randistr{\x}{\y}{100};
			\draw (0,0) rectangle (\x, \y);
			\node[below] at (.5*\x,\y){$root cell$};
		\end{tikzpicture}
		\qquad
		\begin{tikzpicture}[scale=0.33]
			\pgfmathsetseed{2};
			\randistr{\x}{\y}{100};
			\draw (0,0) rectangle (\x * \spx, \y);
			\node[below] at (\spx * 0.5 * \x,\y){$cell1$};
			
			\draw (\x  * \spx, 0) rectangle (\x, \y);
			\node[below] at (0.63 *\x,\y){$cell2$};
			
			\vspace{2mm}
			
		\end{tikzpicture}
		\qquad
		\begin{tikzpicture}[scale=0.33]
			\pgfmathsetseed{2};
			\randistr{\x}{\y}{100};
			\draw (0,0) rectangle (\x * \spx, \y);
			\node[below] at (\spx * 0.5 * \x,\y){$cell1$};
			
			\draw (\x * \spx,0) rectangle (\x, \y * \spyb);
			\node[below] at (.75*\x,\y * \spyb){$cell2$};
			
			\draw (\x * \spx,\y * \spyb) rectangle (\x, \y);
			\node[below] at (.75*\x,\y){$cell3$};
			
			\vspace{2mm}
			
		\end{tikzpicture}
	\end{center}
	\caption{Building a tree of depth 3 using the ORB algorithm }
\end{figure}

\begin{figure}[H]
	\Tree[.rootcell [.cell1 ]
	[.cell2 [.cell3  ]
	[.cell4 ]]]
	\caption{Tree datastructure of depth 3}
\end{figure}

\subsection{Pseudocode}

We always choose the dimension which is the biggest to perform the next cut. This has the advantage of yielding domains which are of a shape closely approximating a square. Square domains are of great importance for the performance and efficiency of the FFM (Fast Multipole Method).

\begin{algorithm}[H]
	\caption{The ORB main routine}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{orb}{$cornerA, cornerB ,particles, np$}
		\State $size = cornerB - cornerA$
		\State $axis = maxIndex(size.x, size.y, size.z)$ \Comment{Get index of max size}
		
		\State $left = cornerA[axis]$
		\State $right = cornerB[axis]$
		\newline
		\State $cut = cut(left, right, axis, particles)$
		\State $mid = reshuffle(split, axis, particles)$
		\newline
		
		\TODO{Finish this}
		\State \Call {orb}{cornerA, cornerB}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Find cut algorithm}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{cut}{$left, right, axis ,particles$}
		\State $nLeft = 0$
		\State $split = (right - left) / 2 + left $ \Comment{Initial guess}
		\While{$abs(nLeft - particles.length/2) > 1 $}
		\State $split = (right - left) / 2 + left $
		\State $nLeft\gets sum(particles[:,axis] < split)$
		\If{$nLeft <= particles.len / 2$}
		\State $left = split$
		\Else 
		\State $right = split$
		\EndIf
		\EndWhile\label{euclidendwhile}
		\State \Return $split$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

% Hoar partitioning
\begin{algorithm}[H]
	\caption{Reshuffle algorithm}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{reshuffle}{$split, axis ,particles$}
		\State $i = 0$
		\State $j = particles.lenghth - 1$
		
		\While{$i < j$}
		\If{$particles[i,axis] < split$}
		\State $i = i + 1$
		\ElsIf{$particles[j,axis] < split$}
		\State $j = j - 1$
		\Else
		\State $tmp = particles[i,:]$
		\State $particles[i,:] = particles[j,:]$
		\State $particles[j,:] = tmp$
		\EndIf
		\EndWhile\label{euclidendwhile}
		
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\vspace{5mm}


\section{Theoretical analysis of ORB speedup by GPU acceleration}

In order to have a broad idea what speedup we can expect by implementing a GPU version, we first need to understand which parts of the code are suited to be done on the GPU. The GPU profits from a highly parallelized architecture, this means that it performs exceptionally when vectorization can be applied to large set of values. But the GPU has another strength over the CPU, which may be lesser known. The memory bandwidth between the GPU and its memory is much greater than the memory bandwidth between the CPU and GPU. Since the time used for calculations is a lot smaller due the algorithm only requiring few and simple operations and is negligible in comparison to memory loading times, we omit speed-ups which are introduced by parallelizing calculations and instead focus on the memory access times. Thus for the section of theoretical analysis we assume that the waiting function is always $w(p) = 0$.

\TODO{Find tflops}

% Compare flops with speed of memory
% Add small experiment
% Memory Bandwidth Cliff


\subsection{Assumptions} 
\begin{itemize}
	\item 
	One particle requires 32 bytes of storage. Where the XYZ coordinates require $4\times3 = 12$ bytes of storage, the rest is used for additional information.
	
	\item
	A single iteration of the binary cut algorithm requires $32 \times N$ operations.
	
	\item 
	All particles are assumed to be stored in the CPU memory initially.
	
	\item
	We perform the analysis on one billion particles. This means we need 32 GB storage for all the particles and 12 GB for only the XYZ positions. This translates to $10^9 \times 4 bytes = 4 GB$, assuming floating point precision. 

\end{itemize}

\subsection{Runtime estimations}

\TODO{Add introduction}

\subsubsection{Naive implementation}

To analyze the runtime for the CPU we are using Equation \ref{eq:cpu}. The constant 32 represents the iterations which are necessary to find a cut. To find a cut we need to sweep 32 times over the entire array of particles, thus we multiply it by the data size $s$ divided by the cpu memory bandwidth denoted as $B_{CPU}$.

\begin{center}
	\begin{equation}
			32 \times \frac{ s }{B_{CPU}} = t
			\label{eq:cpu}
	\end{equation}
\end{center}

\vspace{5mm}


The Equation \ref{eq:gpu} for the GPU is similar, the only difference being that we use the GPU memory bandwidth $B_{GPU}$ instead of the CPU bandwidth. Furthermore we have to consider the time it takes to send the data from the CPU to the GPU. This adds the terms size divided by CPU to GPU memory bandwidth denoted as $I$. Finally we also need to load the data from the CPU memory to the CPU before we are able to send it. 

\begin{center}
	\begin{equation}
		32 \times \frac{s}{B_{GPU}} + \frac{s}{I}  + \frac{s}{B_{CPU}}= t
		\label{eq:gpu}
	\end{equation}
\end{center}

\vspace{5mm}


\subsubsection{GPU tree building}

Let us now consider an alternative way of computing binary cuts. We will build part of the tree on the GPU itself. This will allow us to reduce the very costly overheads imposed by transfering data from the CPU to the GPU. The maximum number of cuts we can perform is 32. After 32 cuts we have $2^{32}$ domains, and have reached the precision limit of the floating point positions. Note that in this case we also need to send back the domain information from the GPU to the CPU. For 

\begin{center}
	\begin{equation}
		32 \times 32 \times \frac{s}{B_{CPU}} = t
		\label{eq:cputree}
	\end{equation}
\end{center}

\begin{center}
	\begin{equation}
		32 \times 32 \times \frac{s}{B_{GPU}} + 2 \times \frac{s}{I} = t
		\label{eq:gputree}
	\end{equation}
\end{center}


\subsubsection{Batch loading}

Finally we can also try to mitigate the overheads introduced by CPU to GPU communication by sending small batches, such that the GPU can already start processing the data, before the data transfer was completed. Lets denote the number of batches by $b$. For the sake of simplicity we only make a single cut and thus can reuse Equation \ref{eq:cpu} for the CPU version. For the GPU we now omit the term for memory access from the GPU. Because we can already start processing data in parallel when the first batch arrived and $B_{GPU} > I$ holds for every modern hardware. Thus the only overhead we still get is $\frac{s \div b}{B_{GPU}}$ for the very last batch, but as we can just increase b in relation to N, this becomes negligible. Note that this only counts for the very first iteration, thus the only difference we have is the constant of 31 and not 32. 

\begin{center}
	\begin{equation}
		31 \times 32 \times \frac{s}{B_{GPU}} + 2 \times \frac{s}{I} = t
		\label{eq:gpubatch}
	\end{equation}
\end{center}

Due to possible overhead and simply no real improvement over the naive GPU version, we will omit the analysis using this method.

\subsubsection{Data compression}

Since for most computer the Interconnect bandwidth is greatly smaller than the Memory Bandwidth, we could potentially also think about compressing the particle data. This would increase computational costs, but would allow us to store a higher amount of particles and reduce the cost which comes from loading the particles from memory as well as sending the data over the Interconnect.

\begin{itemize}
	\item Why do we even use floats in the first place? wouldn't integers suit better since precision is uniformly equal?
	\item Reduce transfered number of bits, sacrifice precision?
\end{itemize}

\subsection{Datapoints of supercomputers}

We will compare the performance with the datapoints of Piz Daint, Summit and Alps which is a part of Eiger. We choose Piz Daint because we have the possibility to test the Code on its systems. We choose Eiger because we can also access its systems and it will give us a good reference values, due to it being a non hybrid supercomputer, meaning the nodes to not have GPU. Finally we analyze its performance on Summit, as its at the time of writing this thesis, one of the most capable supercomputers in the world.

\small
\begin{figure}[H]
	\begin{center}
		\begin{tabular}{ c c c c }
			& Piz Daint \cite{piz_daint} & Summit & Alps (Eiger) \\ 
			\hline
			Number of Nodes & 5704 & 4608 & 1024\\
			CPU Mem. Cap. & 64 GB & 256 GB $\times$ 2  \\   
			CPU Model & Intel E5-2690 v3 & IBM POWER9 $\times$ 2 & AMD EPYC 7742 \\
			CPU Mem. Bandw.  & 68 GB/s & 170 GB/s & 204.8 GB/s x 2	\\
			GPU Model & NVIDIA P100 & NVIDIA V100s  $\times$ 6 & None \\
			GPU Mem. Cap. & 16 GB & 16 GB $\times$ 6 & -\\
			GPU Mem. Bandw. & 732 GB/s & 900 GB/s & -\\
			Interconnect Bandw. & 32 GB/s & 50 GB/s & -\\
		\end{tabular}
	\end{center}
\caption{Datapoints of Supercomputers}
\label{fig:datapoints}
\end{figure}

\normalfont
\subsection{Piz Daint} 

Let us plugin the values from Figure \ref{fig:datapoints} into the corresponding formulas \ref{eq:cpu}, \ref{eq:gpu}, \ref{eq:cputree} and \ref{eq:gputree}.

\subsubsection{Naive implementation}
The naive implementation yields the following speeds for the naive CPU  implementation:

\begin{center}
	\begin{equation}
		32 \times \frac{ 12 GB }{68 GB/s} = 5.65s
	\end{equation}
\end{center}

And the corresponding GPU implementation:
\begin{center}
	\begin{equation}
		32 \times \frac{12 GB}{732 GB/s} + \frac{12 GB}{32 GB/s}  + \frac{12 GB}{68 GB/s} = 1.08s
	\end{equation}
\end{center}

This yields in a speed of:
\begin{center}
	\begin{equation}
		\frac{5.65s}{1.08} = 5.25
	\end{equation}
\end{center}


\subsubsection{GPU Tree Building}

\begin{center}
	\begin{equation}
		32 \times 32 \times \frac{ 12 GB }{68 GB/s} = 180s
	\end{equation}
\end{center}

And the corresponding GPU implementation:
\begin{center}
	\begin{equation}
		32 \times 32 \times \frac{12 GB}{732 GB/s} + 2 \times \frac{12 GB}{32 GB/s}  + \frac{12 GB}{68 GB/s} = 17.71s
	\end{equation}
\end{center}

This yields in a speed of:
\begin{center}
	\begin{equation}
		\frac{180s}{17.71} = 10.2
	\end{equation}
\end{center}

\vspace{5mm}


\subsection{Summit}

Let us plugin the values from Figure \ref{fig:datapoints} into the corresponding formulas \ref{eq:cpu}, \ref{eq:gpu}, \ref{eq:cputree} and \ref{eq:gputree}.

\subsubsection{Naive implementation}
The naive implementation yields the following speeds for the naive CPU  implementation:

\begin{center}
	\begin{equation}
		32 \times \frac{ 12 GB }{170 GB/s} = 2.56s
	\end{equation}
\end{center}

And the corresponding GPU implementation:
\begin{center}
	\begin{equation}
		32 \times \frac{12 GB}{900 GB/s} + \frac{12 GB}{50 GB/s}  + \frac{12 GB}{170 GB/s} = 0.74s
	\end{equation}
\end{center}

This yields in a speed of:
\begin{center}
	\begin{equation}
		\frac{2.56s}{0.74s} = 3.06
	\end{equation}
\end{center}


\subsubsection{GPU Tree Building}

\begin{center}
	\begin{equation}
		32 \times 32 \times \frac{ 12 GB }{170 GB/s} = 70.28s
	\end{equation}
\end{center}

And the corresponding GPU implementation:
\begin{center}
	\begin{equation}
		32 \times 32 \times \frac{12 GB}{900 GB/s} + 2 \times \frac{12 GB}{50 GB/s}  + \frac{12 GB}{170 GB/s} = 14.2s
	\end{equation}
\end{center}

This yields in a speed of:
\begin{center}
	\begin{equation}
		\frac{70.28s}{14.2s} = 4.95
	\end{equation}
\end{center}


\vspace{5mm}


\subsection{Eiger}

\begin{center}
	\begin{equation}
		32 \times 32 \times \frac{ 12 GB }{204.8 GB/s} = 60s
	\end{equation}
\end{center}

\subsection{Conclusion}

We conclude that the GPU version with GPU Tree building enabled yields the best speedup and also performs a lot better than the CPU version. Also we can observe that the speedup is bounded by $\frac{B_{GPU}}{B_{CPU}}$/ \TODO{Add reasoning}

\section{Implementation}

\subsection{Data-structure}

\subsubsection{Tree}

The data-structure which stores the entire three needs to have following properties:

\begin{itemize}
	\item store domain information and provide access to left and right child cells
	\item Allow highly unbalanced trees
\end{itemize}

The most naive approach is to use a struct for each cell which stores information about the domain and keeps a pointer to the left and right cell. However since we in high performance systems, synchronizing pointers can be a bit of hassle, furthermore PKDGrav implements distributed arrays. Thus we can store all domain information in an array and simply keep track of the indices of its child cells. This can even be reduced further, since the right child cell is being created by the same process as the left child was. Thus we can assign the information about the right child in the positions left + 1 and only keep track of the index of the left child.

The minimal amount of data we need to store for each cell is its exact boundaries, the begin and end index in the particles array, and finally a index of the left child. If we sketch out two cells as a memory layout it looks as follows:
\begin{figure}[H]
	\begin{center}
		\begin{bytefield}{24}
			\begin{rightwordgroup}{cell 0}
				\memsection{0}{32}{2}{lower x}\\
				\memsection{32}{64}{2}{lower y}\\
				\memsection{64}{96}{2}{lower z}\\
				\memsection{96}{128}{2}{upper x}\\
				\memsection{128}{160}{2}{upper y}\\
				\memsection{160}{192}{2}{upper z}\\
				\memsection{192}{224}{2}{begin}\\
				\memsection{224}{256}{2}{end}\\
				\memsection{256}{288}{2}{left child}\\
			\end{rightwordgroup}\\
			\begin{rightwordgroup}{cell 1}
				\memsection{288}{320}{2}{lower x}\\
				\memsection{320}{352}{2}{lower y}\\
				\memsection{352}{384}{2}{lower z}\\
				\memsection{384}{416}{2}{upper x}\\
				\memsection{416}{448}{2}{upper y}\\
				\memsection{448}{480}{2}{upper z}\\
				\memsection{480}{512}{2}{begin}\\
				\memsection{512}{544}{2}{end}\\
				\memsection{544}{576}{2}{left child}\\
			\end{rightwordgroup}\\
			
		\end{bytefield}
	\end{center}
\end{figure}

Note that in theory we could only store the split position and axis for each particle, which would only require 32 bits for the positions and 2 bits (for 3D) for the axis. However the exact domain information can then only be achieved by traversing the three starting from the root. We can however make use of this data-structure when the communication overhead is greater than the costs of reconstructing the datastrucutre. Thus this storage version can be thought of as a lossless data compression of the tree.
\subsubsection{Particles}

The particles need to be stored in an array. We can implement the array using c style arrays, std::array or even an std::vector. The disadvantage of std::array and std::vector is their speed. On the other c-style arrays are limited in terms of multi dimensional indexing and boundary checking. Thus we make use of the library blitz++ which combines the speed of C-style arrays with the functionality of the std::vector. Furthermore blitz++ uses the same memory layout we would expect from a C-style array which looks as follows:

\begin{figure}[H]
	\begin{center}
		\begin{bytefield}{24}
			\begin{rightwordgroup}{particle 0}
				\memsection{0}{32}{4}{x}\\
				\memsection{32}{64}{4}{y}\\
				\memsection{64}{96}{4}{z}\\
			\end{rightwordgroup}\\
			\begin{rightwordgroup}{particle 1}
				\memsection{96}{128}{4}{x}\\
				\memsection{128}{160}{4}{y}\\
				\memsection{160}{192}{4}{z}\\
			\end{rightwordgroup}\\
		
		\end{bytefield}
	\end{center}
\end{figure}

\TODO{Data compression notice here?}


\subsection{Parallel CPU Version}

To parallelize the ORB algorithms, we first need to think about which parts of the algorithm can be parallelized and yield in a performance improvement. The most obvious choice for parallelization is the counting part. In this case the main thread (rank 0) will send a cut position and each processor counts how many particles there are on the left of this cut positions. It does so in a designated range of the entire particle array, which is unique to each rank.

Since we reshuffle the particles array after each cut we have found, we also need to broadcast the newly rearranged particles array the all non rank 0 threads.

Another strategy we can use, as soon as we have created two domains, rank 0 can process one domain and rank 1 can process the other domain. This has the advantage that not only the counting part, but also the costly reshuffling part of the ORB algorithm can be parallelized. Furthermore the amount of data which needs to be communicated between the different threads is smaller. This is due to the fact that as soon as we found our first cut, only half the array of particles 

\vspace{5mm}

The reason why it so difficult to parallelize the reshuffling part is that it is very difficult to do in place. If we cannot perform it in place, we will probably have to use twice the memory capacity, which is will increase the maximum possible $N$ by a factor of two, which is not ideal.

\TODO{Extend section}

\subsection{GPU parallelization of ORB using CUDA }

% Use for reduction explanation https://texample.net/tikz/examples/database-decimation-process/
\bibliographystyle{plain}
\bibliography{reference}

\end{document}
