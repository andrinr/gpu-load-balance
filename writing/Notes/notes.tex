\documentclass[]{article}
\usepackage{algpseudocode}

\usepackage{algorithm}
\usepackage{float}% 
\usepackage{tikz}
\usepackage{mymacros}
\usepackage{qtree}
%opening
\title{Notes}
\author{Andrin Rehmann}

\begin{document}

\maketitle

\section{Introduction}


The N-Body technique has been used for decades to simulate the Universe so we can compare theory with observations. This technique uses ``particles'' or ``bodies'' to sample phase space, and as gravity operates over infinite distance it is necessary to consider all pair-wise interactions which makes a naive implementation $\mathcal{O}(n^2)$.
It is clear that this does not scale particularly well with large particle counts.

One common approach is to decompose the particles into a tree structure and multipole expansions of the particles in each tree cell to approximate the forces. This reduces the complexity of the algorithm to $\mathcal{O}(n\log{}n)$. More recently the Fast Multipole Method (FMM) has gained wider option primarily due to the extremely large sizes of modern simulations. This technique further reduces the complexity to $\mathcal{O}(n)$!

The computational effort required in modern N-Body simulations can thus be split into three categories:

\begin{itemize}
	\item \textbf{Load Balancing:} Distribute particles equally among nodes with regards to memory.
	\item \textbf{Tree Building:} Build tree on each node for accelerated force calculation and integration.
	\item \textbf{Force calculation and integration:} Calculate forces between particles and apply them.
\end{itemize}

Before the implementation of FMM into codes, and particularly before the era of modern accelerated computing (e.g., SIMD vectorization and GPU computing) the forces calculations dominated the computational cost. In more recent simulations, each category is about one third of the total calculation time\cite{2017ComAC...4....2P}. This makes the tree building and load balancing subjects to great performance gains, since GPU acceleration is usually not exploited. 

This project proposes to implement ORB with the CUDA API to accelerate load balancing.

\section{ORB Algorithm}


%https://de.overleaf.com/learn/latex/TikZ_package
%https://texample.net/tikz/examples/
The ORB Algorithm can be used to partition a multi dimensional domain into subdomains based on spatial proximity. The advantage of subdividing the domain into smaller subdomains is two folds:

\begin{itemize}
	\item For simulation code and more specifically for the Fast Multipole Method these domains can be leveraged to greatly accelerate the simulation time. 
	\item The workload can be equally distributed among nodes and processors in large computing systems.
\end{itemize} 


The general idea of the ORB algorithm is to split a $k$ dimensional domain containing $N$ particles into $np$ subdomains using a recursive procedure. This can be repeated until the number of smallest subdomains is equal to the total count of processors or nodes. The final goal is an equal distribution of the workload and data, thus the subdomain sizes should be chosen such that the number of particles is equal among nodes / processors.

\subsection{Memory and Workload Balancing}
For the ORB algorithms itself, the relation between the number of processors and the workload holds steady. But for the simulation certain particles require more timesteps, thus more computational effort, than others. This is mostly due to higher accelerations and greater proximity to strong gravitational influences. This in turn requires a higher accuracy to mitigate errors. If we try to incorporate these effects, we will end up with partitions which are more equal in terms of workload, but less equal in terms of memory. 

\vspace{5mm}
If we consider a system with various nodes, the approach is usually to distribute 
Unequal balancing of the memory in favor of a more equal balance in terms of workload, will results in 
 For the sake of simplicity, we will assume it is sensible to simply partition the domain into equally sized subdomains.

TODO: extend reasoning

\subsection{ORB for power of twos}

Let us first make the assumption that $np$ is a power of two. In this case we can search for a cut such that 50\% of the particles are on the left side and likewise on the right side of the cut. The position of the cut can be found in $O(log(N))$ using a binary search. We can introduce an even tighter bound when assuming each particle is being stored as a 32 bit precision number. In this case we have reached the maximum precision after $O(32)$ iterations. After such a cut has been found, we can create two new subdomains and recursively apply the algorithm until we have reached the desired number of subdomains $np$. 

\vspace{5mm}

\def\x{8}
\def\y{8}

\def\spx{0.45}
\def\spya{0.5}
\def\spyb{0.6}
\begin{figure}[H]
\begin{center}
	\begin{tikzpicture}[scale=0.33]
		\pgfmathsetseed{2};
		\randistr{\x}{\y}{100};
		\draw (0,0) rectangle (\x, \y);
		\node[below] at (.5*\x,\y){$root cell$};
	\end{tikzpicture}
	\qquad
	\begin{tikzpicture}[scale=0.33]
		\pgfmathsetseed{2};
		\randistr{\x}{\y}{100};
		\draw (0,0) rectangle (\x * \spx, \y);
		\node[below] at (.25*\x,\y){$cell1$};
		
		\draw (\x  * \spx, 0) rectangle (\x, \y);
		\node[below] at (.75*\x,\y){$cell2$};
		
		\vspace{2mm}
		
	\end{tikzpicture}
	\qquad
	\begin{tikzpicture}[scale=0.33]
		\pgfmathsetseed{2};
		\randistr{\x}{\y}{100};
		\draw (0,0) rectangle (\x * \spx, \y * \spya);
		\node[below] at (.25*\x,\y * \spya){$cell2$};
		
		\draw (0,\y * \spya) rectangle (\x * \spx, \y);
		\node[below] at (.25*\x,\y){$cell3$};
		
		\draw (\x * \spx,0) rectangle (\x, \y * \spyb);
		\node[below] at (.75*\x,\y * \spyb){$cell4$};
		
		\draw (\x * \spx,\y * \spyb) rectangle (\x, \y);
		\node[below] at (.75*\x,\y){$cell5$};
		
		\vspace{2mm}
	
\end{tikzpicture}
\end{center}
\caption{Building a tree of depth 3 using the ORB algorithm }
\end{figure}

For a clearer terminology, we will from now on refer to a domain as a cell. Furthermore the subdomain of a domain is the child cell of a cell, and likewise we also have the concept of a parent cell.

\vspace{5mm}

When each cell keeps track of its child cells, we have built a tree datastructure. In this tree each cell represents a node, the root node is the cell which encompasses the entire space and particles and finally its leaves can be assigned to a processor / node. Note that the tree is not balanced, but each node has either two children or is a leaf. 

\begin{figure}[H]
\Tree[.rootcell [.cell1 [.cell2 \textit{P0} ]
[.cell3 \textit{P1} ]]
[.cell4 [.cell5 \textit{P2} ]
[.cell5 \textit{P3} ]]]
\caption{Tree datastructure of depth 3}
\end{figure}

\subsection{ORB in the general case}

Let us now consider an example where the desired number of leaf cells is not a power of two, for example 7. In this case our first split should not be a 

\def\spx{0.36}
\def\spya{0.5}
\def\spyb{0.6}
\begin{figure}[H]
	\begin{center}
		\begin{tikzpicture}[scale=0.33]
			\pgfmathsetseed{2};
			\randistr{\x}{\y}{100};
			\draw (0,0) rectangle (\x, \y);
			\node[below] at (.5*\x,\y){$root cell$};
		\end{tikzpicture}
		\qquad
		\begin{tikzpicture}[scale=0.33]
			\pgfmathsetseed{2};
			\randistr{\x}{\y}{100};
			\draw (0,0) rectangle (\x * \spx, \y);
			\node[below] at (\spx * 0.5 * \x,\y){$cell1$};
			
			\draw (\x  * \spx, 0) rectangle (\x, \y);
			\node[below] at ((1.0 - \spx * 0.5) *\x,\y){$cell2$};
			
			\vspace{2mm}
			
		\end{tikzpicture}
		\qquad
		\begin{tikzpicture}[scale=0.33]
			\pgfmathsetseed{2};
			\randistr{\x}{\y}{100};
			\draw (0,0) rectangle (\x * \spx, \y * \spya);
			\node[below] at (.25*\x,\y * \spya){$cell2$};
			
			\draw (0,\y * \spya) rectangle (\x * \spx, \y);
			\node[below] at (.25*\x,\y){$cell3$};
			
			\draw (\x * \spx,0) rectangle (\x, \y * \spyb);
			\node[below] at (.75*\x,\y * \spyb){$cell4$};
			
			\draw (\x * \spx,\y * \spyb) rectangle (\x, \y);
			\node[below] at (.75*\x,\y){$cell5$};
			
			\vspace{2mm}
			
		\end{tikzpicture}
	\end{center}
	\caption{Building a tree of depth 3 using the ORB algorithm }
\end{figure}

\subsection{Pseudocode}

\begin{algorithm}[H]
	\caption{The ORB main routine}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{orb}{$cornerA, cornerB ,particles, np$}
		\State $size = cornerB - cornerA$
		\State $axis = maxIndex(size.x, size.y, size.z)$ \Comment{Get index of max size}
		
		\State $left = cornerA[axis]$
		\State $right = cornerB[axis]$
		\newline
		\State $cut = cut(left, right, axis, particles)$
		\State $mid = reshuffle(split, axis, particles)$
		\newline
		
		TODO
		\State \Call {orb}{cornerA, cornerB}
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Find cut algorithm}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{cut}{$left, right, axis ,particles$}
		\State $nLeft = 0$
		\State $split = (right - left) / 2 + left $ \Comment{Initial guess}
		\While{$abs(nLeft - particles.length/2) > 1 $}
		\State $split = (right - left) / 2 + left $
		\State $nLeft\gets sum(particles[:,axis] < split)$
		\If{$nLeft <= particles.len / 2$}
		\State $left = split$
		\Else 
		\State $right = split$
		\EndIf
		\EndWhile\label{euclidendwhile}
		\State \Return $split$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
	\caption{Reshuffle algorithm}\label{euclid}
	\begin{algorithmic}[1]
		\Procedure{reshuffle}{$split, axis ,particles$}
		\State $i = 0$
		\State $j = particles.lenghth - 1$
		
		\While{$i < j$}
		\If{$particles[i,axis] < split$}
		\State $i = i + 1$
		\ElsIf{$particles[j,axis] < split$}
		\State $j = j - 1$
		\Else
		\State $tmp = particles[i,:]$
		\State $particles[i,:] = particles[j,:]$
		\State $particles[j,:] = tmp$
		\EndIf
		\EndWhile\label{euclidendwhile}
		
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\vspace{5mm}

TODO

This decomposition step is very straight forward, as long as the number of final subdomains $s$ is a power of two. Otherwise we will always end 

We always choose the dimension which is the biggest to perform the next cut. This has the advantage of yielding domains which are of a shape closely approximating a square. Square domains are of great importance for the performance and efficiency of the FFM (Fast Multipole Method).

We assume positions are stored as a 32 bit precision float which is reasonable for astrophysical calculations. 

The reshuffle procedure takes the array of particles and rearanges the elements such that all element left of the split are on the one side, and the others are on the other side. 

TODO: more detailed, add section about special cases, add section about tree datastructure
\section{Theoretical Analysis of ORB}

The goal is to have a general idea how and if the performance of the ORB algorithm will greatly benefit from a GPU implementation. We will compare the performance with the datapoints of Piz Daint, Summit and Alps which is a part of Eiger. 

\subsection{Assumptions} 
\begin{itemize}
	\item 
	One particle requires 32 bytes of storage. Where the XYZ coordinates require $4\times3 = 12$ bytes of storage, the rest is used for additional information.
	
	\item
	A single iteration of the binary cut algorithm requires $32 \times N$ operations.
	
	\item 
	All particles are assumed to be stored in the CPU memory initially.
	
	\item
	We perform the analysis on one billion particles. This means we need 32 GB storage for all the particles and 12 GB for only the XYZ positions.
	
	\item
	We do not consider calculation times as memory access times  outweigh them. The operations are simple comparison  which require only very little cycles on the chip. 
\end{itemize}

\subsection{Datapoints of supercomputers}

\small
\begin{center}
	\begin{tabular}{ c c c c }
		& Piz Daint HYBRID \cite{piz_daint} & Summit & Alps (Eiger) \\ 
		\hline
		Number of Nodes & 5704 & 4608 & 1024\\
		CPU Mem. Cap. & 64 GB & 256 GB $\times$ 2  \\   
		CPU Model & Intel E5-2690 v3 & IBM POWER9 $\times$ 2 & AMD EPYC 7742 \\
		CPU Mem. Bandw.  & 68 GB/s & 170 GB/s & 204.8 GB/s x 2	\\
		GPU Model & NVIDIA P100 & NVIDIA V100s  $\times$ 6 & None \\
		GPU Mem. Cap. & 16 GB & 16 GB $\times$ 6 & -\\
		GPU Mem. Bandw. & 732 GB/s & 900 GB/s & -\\
		Interconnect Bandw. & 32 GB/s & 50 GB/s & -\\
	\end{tabular}
\end{center}
\normalfont
\subsection{Piz Daint} 
When executing the algorithm on the CPU, for a single iteration of the binary cut algorithm we perform a scan over an axis of all particle positions. This translates to $10^9 \times 4 bytes = 4 GB$.
On the CPU this results in a memory access time of 
\begin{center}
	$32 \times \frac{ 4 GB}{68 GB/s} = 1.8 s$ 
\end{center}

If we do the same on the GPU we get an access time of:
\begin{center}
	$32 \times \frac{4 GB}{732 GB/s} + \frac{4 GB}{32 GB/s} = 0.3 s$ 
\end{center}

This results in a theoretical speedup of:

\begin{center}
	$\frac{1.8}{0.3} = 6$
\end{center}

Where the first part are the 32 iterations of the memory sweep and the second part are the transfer times from CPU to GPU. We only need to transfer back a single number, which is then used to termine the next split position. Thus transfer times from the GPU to CPU are negligible. It is clear that the transfer times from the CPU to the GPU dominate the runtime.

Let us now consider an alternative way of computing binary cuts. We will build part of the tree on the GPU itself. We have a shared memory size of 64 KB. Since storing the information about the tree requires only very little storage, we are not limited by memory and can perform 32 cuts on the GPU. Note that in this case we need to load all three coordinates to the GPU. We omit the domain data transfer from the GPU to the CPU as it can be stored in very little memory. 

\begin{center}
	$32 \times 32 \times \frac{ 4 GB}{68 GB/s} = 60.2 s$ 
\end{center}

\begin{center}
	$32 \times 32 \times \frac{4 GB}{732 GB/s} + \frac{12 GB}{32 GB/s} + \frac{4 GB}{32 GB/s} = 6 s$ 
\end{center}

\begin{center}
	$\frac{60.2}{6} = 10$ 
\end{center}

\subsection{Summit}

TODO

\subsection{Eiger}

TODO 

\section{Implementation}

\subsection{Data-structure}

The data-structure which stores the entire three needs to have following properties:

\begin{itemize}
	\item store domain information and provide access to left and right child cells
	\item Allow highly unbalanced trees
\end{itemize}

The most naive approach is to use a struct for each cell which stores information about the domain and keeps a pointer to the left and right cell. However since we in high performance systems, synchronizing pointers can be a bit of hassle, furthermore PKDGrav implements distributed arrays. Thus we can store all domain information in an array and simply keep track of the indices of its child cells. This can even be reduced further, since the right child cell is being created by the same process as the left child was. Thus we can assign the information about the right child in the positions left + 1 and only keep track of the index of the left child.

Since we are building a tree and we 


PkdGrav uses concept of distributed arrays, makes use of logical indexed, thus we do not use pointers. Pointers also take up more memory. 

Standard array are not high performance. Has an overhead. Standard vector is not too bad. Blitz arrays. 
ARgue about vector, array std::vector

\subsection{CPU parallelization of ORB}

To parallelize the ORB algorithms, we first need to think about which parts of the algorithm can be parallelized and yield in a performance improvement. The most obvious choice for parallelization is the counting part. In this case the main thread will send a split and the array of particles is distributed among all the processy
Use OpenMPI / MPich, btoh same standard
Reshuffling of arrays cannot be done in place.

There are several variants on how to parallelize the CPU parallelized version of the ORB. Since it is a recursive algorithm 


\subsection{CPU parallelization of ORB using CUDA }
\bibliographystyle{plain}
\bibliography{reference}

\end{document}
