\documentclass[]{article}

%opening
\title{Notes}
\author{Andrin Rehmann}

\begin{document}

\maketitle

\section{Datapoints of supercomputers}

\begin{center}
	\begin{tabular}{ c c c }
		& Piz Daint HYBRID \cite{piz_daint} & Summit \\ 
		\hline
		Number of Nodes & 5704 & 4608\\
		CPU Mem. Cap. & 64 GB & 256 GB $\times$ 2  \\   
		CPU Model & Intel Xeon E5-2690 v3 & IBM POWER9 $\times$ 2 \\
		CPU Mem. Bandwith  & 68 GB/s & 170 GB/s\\
		CPU Number of Cores & 12 \\
		GPU Model & NVIDIA Tesla P100 & NVIDIA Volta V100s $\times$ 6 \\
		GPU Mem. Cap. & 16 GB & 16 GB $\times$ 6\\
		GPU Mem. Bandwith & 732 GB/s & 900 GB/s \\
		GPU Interconnect Bandwidth & 32 GB/s & 50 GB/s \\
		GPU Single Prec. Perf. & 9.3 tFlops
	\end{tabular}
\end{center}

\subsection{Terminology}

\begin{enumerate}
	\item Memory Bandwidth: read / write speed of semiconductor memory
	\item 

\end{enumerate}

\section{Analysis}

\begin{itemize}
	\item 
	One particle requires 32 bytes of storage. Where the XYZ coordinates require $4\times3 = 12$ bytes of storage, the rest is used for additional information.
	
	\item
	A single iteration of the binary cut algorithm requires $32 \times N$ operations.
	
	\item 
	All particles are assumed to be stored in the CPU memory initially.
	
	\item
	We perform the analysis on one billion particles. This means we need 32 GB storage for all the particles and 12 GB for only the XYZ positions.
\end{itemize}


\subsection{Piz Daint} 
When executing the algorithm on the CPU, for a single iteration of the binary cut algorithm we perform a scan over an axis of all particle positions. This translates to $10^9 \times 4 bytes = 4 GB$.
On the CPU this results in a memory access time of 
\begin{center}
	$32 \times \frac{ 4 GB}{68 GB/s} = 1.8 s$ 
\end{center}

If we do the same on the GPU we get an access time of:
\begin{center}
	$32 \times \frac{4 GB}{732 GB/s} + 2 \times \frac{4 GB}{32 GB/s} = 0.38 s$ 
\end{center}

Where the first part are the 32 iterations of the memory sweep and the second part are the transfer times from CPU to GPU and from CPU to GPU. It is clear that the transfer times from the CPU to the GPU dominate the runtime.

Let us now consider an alternative way of computing binary cuts. We will build part of the tree on the GPU itself. Since we have a shared memory size of 64 KB and we assume a single tree cell requires 64 bytes of information, we can perform 6 binary cuts on GPU. Note that in this we need to load all three coordinates to the GPU which translates to following formulas:

\begin{center}
	$6 \times 32 \times \frac{ 4 GB}{68 GB/s} = 11.3 s$ 
\end{center}

\begin{center}
	$6 \times 32 \times \frac{4 GB}{732 GB/s} + 2 \times \frac{12 GB}{32 GB/s} = 1.8 s$ 
\end{center}

Todo: Consider not only increased datatransfer rates of the GPU but also consider how the GPU might be able to improve the speeds with a higher parallelization.
What about shared memory costs? How to estimate?



\bibliographystyle{plain}
\bibliography{reference}

\end{document}
